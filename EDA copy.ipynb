{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment 3\n",
    "\n",
    "Dataset = IMDB data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"IMDB Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    One of the other reviewers has mentioned that ...\n",
       "1    A wonderful little production. <br /><br />The...\n",
       "2    I thought this was a wonderful way to spend ti...\n",
       "3    Basically there's a family where a little boy ...\n",
       "4    Petter Mattei's \"Love in the Time of Money\" is...\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"review\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50000</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>49582</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Loved today's show!!! It was a variety and not...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>5</td>\n",
       "      <td>25000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   review sentiment\n",
       "count                                               50000     50000\n",
       "unique                                              49582         2\n",
       "top     Loved today's show!!! It was a variety and not...  positive\n",
       "freq                                                    5     25000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning and Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review       0\n",
       "sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.isnull().sum().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.loc[dataset['sentiment'] == 'positive','sentiment'] = 1\n",
    "dataset.loc[dataset['sentiment'] == 'negative', 'sentiment'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.tail of                                                   review sentiment\n",
       "0      One of the other reviewers has mentioned that ...         1\n",
       "1      A wonderful little production. <br /><br />The...         1\n",
       "2      I thought this was a wonderful way to spend ti...         1\n",
       "3      Basically there's a family where a little boy ...         0\n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...         1\n",
       "...                                                  ...       ...\n",
       "49995  I thought this movie did a down right good job...         1\n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...         0\n",
       "49997  I am a Catholic taught in parochial elementary...         0\n",
       "49998  I'm going to have to disagree with the previou...         0\n",
       "49999  No one expects the Star Trek movies to be high...         0\n",
       "\n",
       "[50000 rows x 2 columns]>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.tail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert everything to lowercase and remove unwanted characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['review'] = dataset['review'].str.lower().str.replace('[^a-z\\\\s]', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the actors cannot act all dialoague was plagued with bad accents and loss of character channing tatum never moves his lips or changes his facial expression everbr br the story is nothing new at all some kid from the street gets involved in a professional world of dance and it turns his life around that coupled with the whole incident involving the little kid is taken straight from you got served and save the last dance im not saying that those movies were any good either but that is to say that this movie brought nothing new to the tablebr br and the dancing there were only  dance sequences in the entire movie and  of them were taken straight from the commercial perhaps im being overly critical because i am a dancer but maybe thats what needs to be heard channing tatum is not by any means a bboy his little solo in the parking lot had little style technique or any wow factor all of which are part of a street dancers criteria all of the jazz and ballet in the movie had nothing to offer except bad technique and a few acceptable twirls but nothing more the grande finale left me thinking  ok now theyre gonna get serious all the way through the end when i realized it never was going to happenbr br ill admit that im sure it is difficult to make a good dance movie but step up is no exception to the rule you got served with the exception of its inconsistencies with street dance culture at least had the dance aspect save the last dance was garbage and so was just about any musical from the past  years although i was impressed with moulin rouge look to center stage for ballet look to beat street for hiphop'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['review'][4889]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production br br the filmin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically theres a family where a little boy j...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter matteis love in the time of money is a ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  one of the other reviewers has mentioned that ...         1\n",
       "1  a wonderful little production br br the filmin...         1\n",
       "2  i thought this was a wonderful way to spend ti...         1\n",
       "3  basically theres a family where a little boy j...         0\n",
       "4  petter matteis love in the time of money is a ...         1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv('IMDB Dataset_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The class of 'Amazing plot and acting' is: negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harsh\\AppData\\Local\\Temp\\ipykernel_8180\\237264967.py:17: RuntimeWarning: divide by zero encountered in log\n",
      "  self.log_class_probs = {c: np.log(np.sum(y == c) / n_samples) for c in self.classes}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "class LogSpaceMultinomialNaiveBayes:\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha\n",
    "        self.classes = None\n",
    "        self.log_class_probs = None\n",
    "        self.log_word_probs = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        n_samples = len(X)\n",
    "        \n",
    "        # Calculate log class probabilities\n",
    "        self.log_class_probs = {c: np.log(np.sum(y == c) / n_samples) for c in self.classes}\n",
    "        \n",
    "        # Count word occurrences for each class\n",
    "        word_counts = {c: defaultdict(int) for c in self.classes}\n",
    "        for doc, label in zip(X, y):\n",
    "            for word in self.tokenize(doc):\n",
    "                word_counts[label][word] += 1\n",
    "        \n",
    "        # Calculate log word probabilities\n",
    "        self.log_word_probs = {c: {} for c in self.classes}\n",
    "        for c in self.classes:\n",
    "            total_words = sum(word_counts[c].values())\n",
    "            for word, count in word_counts[c].items():\n",
    "                self.log_word_probs[c][word] = np.log((count + self.alpha) / (total_words + self.alpha * len(word_counts[c])))\n",
    "\n",
    "    def predict(self, X):\n",
    "        return [self.predict_single(doc) for doc in X]\n",
    "\n",
    "    def predict_single(self, doc):\n",
    "        scores = {c: self.log_class_probs[c] for c in self.classes}\n",
    "        for word in self.tokenize(doc):\n",
    "            for c in self.classes:\n",
    "                if word in self.log_word_probs[c]:\n",
    "                    scores[c] += self.log_word_probs[c][word]\n",
    "        \n",
    "        # Convert log scores back to linear space for comparison\n",
    "        linear_scores = {c: np.exp(score) for c, score in scores.items()}\n",
    "        return max(linear_scores, key=linear_scores.get)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "# Example usage\n",
    "X_train = [\"This movie was great!\", \"Terrible film, waste of time\", \"I loved it\", \"Boring and predictable\"]\n",
    "y_train = [\"positive\", \"negative\", \"positive\", \"negative\"]\n",
    "\n",
    "clf = LogSpaceMultinomialNaiveBayes()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Test instance\n",
    "S = \"Amazing plot and acting\"\n",
    "prediction = clf.predict_single(S)\n",
    "print(f\"The class of '{S}' is: {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Performance Metrics:\n",
      "Accuracy: 85.19%\n",
      "\n",
      "Detailed Metrics:\n",
      "Accuracy: 0.8519\n",
      "True Positive: 4350\n",
      "True Negative: 4169\n",
      "False Positive: 792\n",
      "False Negative: 689\n",
      "Precision: 0.8459743290548425\n",
      "Recall: 0.8632665211351459\n",
      "\n",
      "Sample Predictions:\n",
      "Review: 'This movie was absolutely fantastic!'\n",
      "Sentiment: Positive\n",
      "\n",
      "Review: 'Terrible film, completely waste of time.'\n",
      "Sentiment: Negative\n",
      "\n",
      "Review: 'An okay movie with some good moments.'\n",
      "Sentiment: Negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from typing import List, Tuple\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class NaiveBayesSentimentClassifier:\n",
    "    def __init__(self, smoothing_param: float = 1.0):\n",
    "        \"\"\"\n",
    "        Initialize Naive Bayes Classifier\n",
    "        \n",
    "        :param smoothing_param: Laplace smoothing parameter to prevent zero probabilities\n",
    "        \"\"\"\n",
    "        self.class_probs = {}  # Prior probabilities of classes\n",
    "        self.word_probs = {}   # Conditional probabilities of words given class\n",
    "        self.vocab = set()     # Unique words in the training data\n",
    "        self.alpha = smoothing_param  # Smoothing parameter\n",
    "    \n",
    "    def preprocess_text(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Preprocess text by converting to lowercase and removing non-alphabetic characters\n",
    "        \n",
    "        :param text: Input text to preprocess\n",
    "        :return: List of processed words\n",
    "        \"\"\"\n",
    "        # Handle potential NaN values\n",
    "        if not isinstance(text, str):\n",
    "            return []\n",
    "        \n",
    "        # Convert to lowercase and remove punctuation\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text.lower())\n",
    "        return text.split()\n",
    "    \n",
    "    def train(self, X: List[str], y: List[int]):\n",
    "        \"\"\"\n",
    "        Train the Naive Bayes classifier\n",
    "        \n",
    "        :param X: List of text documents\n",
    "        :param y: Corresponding class labels (0 or 1)\n",
    "        \"\"\"\n",
    "        from collections import defaultdict\n",
    "        \n",
    "        # Count total documents and documents per class\n",
    "        total_docs = len(X)\n",
    "        class_counts = defaultdict(int)\n",
    "        word_counts = {0: defaultdict(int), 1: defaultdict(int)}\n",
    "        \n",
    "        # Preprocess and count word occurrences\n",
    "        for doc, label in zip(X, y):\n",
    "            class_counts[label] += 1\n",
    "            words = self.preprocess_text(doc)\n",
    "            \n",
    "            for word in words:\n",
    "                word_counts[label][word] += 1\n",
    "                self.vocab.add(word)\n",
    "        \n",
    "        # Calculate prior probabilities (log space)\n",
    "        for label in set(y):\n",
    "            self.class_probs[label] = np.log(class_counts[label] / total_docs)\n",
    "        \n",
    "        # Calculate conditional probabilities (log space)\n",
    "        self.word_probs = {label: {} for label in set(y)}\n",
    "        vocab_size = len(self.vocab)\n",
    "        \n",
    "        for label in set(y):\n",
    "            for word in self.vocab:\n",
    "                # Apply Laplace smoothing in log space\n",
    "                word_count = word_counts[label][word]\n",
    "                denominator = class_counts[label] + self.alpha * vocab_size\n",
    "                log_prob = np.log((word_count + self.alpha) / denominator)\n",
    "                self.word_probs[label][word] = log_prob\n",
    "    \n",
    "    def predict(self, document: str) -> int:\n",
    "        \"\"\"\n",
    "        Predict the sentiment of a document\n",
    "        \n",
    "        :param document: Text document to classify\n",
    "        :return: Predicted class (0 or 1)\n",
    "        \"\"\"\n",
    "        words = self.preprocess_text(document)\n",
    "        \n",
    "        # Calculate log probabilities for each class\n",
    "        log_probs = {}\n",
    "        for label in self.class_probs:\n",
    "            # Start with log prior probability\n",
    "            log_prob = self.class_probs[label]\n",
    "            \n",
    "            # Sum log probabilities of words\n",
    "            for word in words:\n",
    "                if word in self.vocab:\n",
    "                    log_prob += self.word_probs[label].get(word, np.log(1 / (len(self.vocab) + 1)))\n",
    "            \n",
    "            log_probs[label] = log_prob\n",
    "        \n",
    "        # Return class with highest probability (convert from log space)\n",
    "        return max(log_probs, key=log_probs.get)\n",
    "    \n",
    "    def evaluate(self, X_test: List[str], y_test: List[int]) -> Tuple[float, dict]:\n",
    "        \"\"\"\n",
    "        Evaluate the classifier's performance\n",
    "        \n",
    "        :param X_test: Test documents\n",
    "        :param y_test: True labels\n",
    "        :return: Accuracy and detailed metrics\n",
    "        \"\"\"\n",
    "        predictions = [self.predict(doc) for doc in X_test]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = sum(p == t for p, t in zip(predictions, y_test)) / len(y_test)\n",
    "        \n",
    "        # Confusion matrix\n",
    "        true_positive = sum((p == 1 and t == 1) for p, t in zip(predictions, y_test))\n",
    "        true_negative = sum((p == 0 and t == 0) for p, t in zip(predictions, y_test))\n",
    "        false_positive = sum((p == 1 and t == 0) for p, t in zip(predictions, y_test))\n",
    "        false_negative = sum((p == 0 and t == 1) for p, t in zip(predictions, y_test))\n",
    "        \n",
    "        return accuracy, {\n",
    "            'accuracy': accuracy,\n",
    "            'true_positive': true_positive,\n",
    "            'true_negative': true_negative,\n",
    "            'false_positive': false_positive,\n",
    "            'false_negative': false_negative,\n",
    "            'precision': true_positive / (true_positive + false_positive) if (true_positive + false_positive) > 0 else 0,\n",
    "            'recall': true_positive / (true_positive + false_negative) if (true_positive + false_negative) > 0 else 0\n",
    "        }\n",
    "\n",
    "def load_imdb_data(filepath: str, test_size: float = 0.2, random_state: int = 42) -> Tuple[List[str], List[int], List[str], List[int]]:\n",
    "    \"\"\"\n",
    "    Load IMDB dataset from CSV file\n",
    "    \n",
    "    :param filepath: Path to the CSV file\n",
    "    :param test_size: Proportion of data to use for testing\n",
    "    :param random_state: Random seed for reproducibility\n",
    "    :return: X_train, y_train, X_test, y_test\n",
    "    \"\"\"\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    # Assuming the CSV has columns 'review' and 'sentiment'\n",
    "    # Sentiment should be binary (0 or 1)\n",
    "    X = df['review'].fillna('').tolist()\n",
    "    y = df['sentiment'].tolist()\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "def main():\n",
    "    # Replace with the path to your IMDB reviews CSV\n",
    "    filepath = 'IMDB Dataset_cleaned.csv'\n",
    "    \n",
    "    try:\n",
    "        # Load the data\n",
    "        X_train, y_train, X_test, y_test = load_imdb_data(filepath)\n",
    "        \n",
    "        # Create and train classifier\n",
    "        clf = NaiveBayesSentimentClassifier()\n",
    "        clf.train(X_train, y_train)\n",
    "        \n",
    "        # Evaluate the classifier\n",
    "        accuracy, metrics = clf.evaluate(X_test, y_test)\n",
    "        \n",
    "        # Print results\n",
    "        print(\"Classifier Performance Metrics:\")\n",
    "        print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "        print(\"\\nDetailed Metrics:\")\n",
    "        for key, value in metrics.items():\n",
    "            print(f\"{key.replace('_', ' ').title()}: {value}\")\n",
    "        \n",
    "        # Optional: Predict on a few sample reviews\n",
    "        sample_reviews = [\n",
    "            \"This movie was absolutely fantastic!\",\n",
    "            \"Terrible film, completely waste of time.\",\n",
    "            \"An okay movie with some good moments.\"\n",
    "        ]\n",
    "        \n",
    "        print(\"\\nSample Predictions:\")\n",
    "        for review in sample_reviews:\n",
    "            prediction = clf.predict(review)\n",
    "            print(f\"Review: '{review}'\\nSentiment: {'Positive' if prediction == 1 else 'Negative'}\\n\")\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{filepath}' not found. Please provide the correct path to your IMDB reviews CSV.\")\n",
    "    except KeyError as e:\n",
    "        print(f\"Error: Missing column in CSV file. {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: python script.py <algorithm> <train_size>\n",
      "Using default: Naive Bayes with 80% training size\n",
      "Last Name, First Name, AXXXXXXXX solution:\n",
      "Training set size: 80 %\n",
      "Classifier type: Naive Bayes\n",
      "Training classifier...\n",
      "Testing classifier...\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 244\u001b[0m\n\u001b[0;32m    241\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 244\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[23], line 201\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    198\u001b[0m     clf\u001b[38;5;241m.\u001b[39mtrain(X_train, y_train)\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting classifier...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 201\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Logistic Regression\u001b[39;00m\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassifier type: Logistic Regression\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[23], line 201\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    198\u001b[0m     clf\u001b[38;5;241m.\u001b[39mtrain(X_train, y_train)\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting classifier...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 201\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m [\u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m X_test]\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Logistic Regression\u001b[39;00m\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassifier type: Logistic Regression\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[23], line 101\u001b[0m, in \u001b[0;36mNaiveBayesClassifier.predict\u001b[1;34m(self, sentence)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, sentence):\n\u001b[1;32m--> 101\u001b[0m     probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmax\u001b[39m(probs, key\u001b[38;5;241m=\u001b[39mprobs\u001b[38;5;241m.\u001b[39mget)\n",
      "Cell \u001b[1;32mIn[23], line 96\u001b[0m, in \u001b[0;36mNaiveBayesClassifier.predict_proba\u001b[1;34m(self, sentence)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# Normalize probabilities\u001b[39;00m\n\u001b[0;32m     95\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(probs\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m---> 96\u001b[0m probs \u001b[38;5;241m=\u001b[39m \u001b[43m{\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprob\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mtotal\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprob\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprobs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m probs\n",
      "Cell \u001b[1;32mIn[23], line 96\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# Normalize probabilities\u001b[39;00m\n\u001b[0;32m     95\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(probs\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m---> 96\u001b[0m probs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mcls\u001b[39m: \u001b[43mprob\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mtotal\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m, prob \u001b[38;5;129;01min\u001b[39;00m probs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m probs\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score, \n",
    "    accuracy_score\n",
    ")\n",
    "import math\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "    def __init__(self):\n",
    "        self.class_probs = {}\n",
    "        self.word_probs = {}\n",
    "        self.vocab = set()\n",
    "        self.classes = None\n",
    "\n",
    "    def _preprocess_text(self, text):\n",
    "        # More robust text preprocessing\n",
    "        # Convert to lowercase\n",
    "        text = str(text).lower()\n",
    "        \n",
    "        # Remove special characters and digits\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        \n",
    "        # Split into words\n",
    "        words = text.split()\n",
    "        \n",
    "        # Optional: Remove very short words (less than 2 characters)\n",
    "        words = [word for word in words if len(word) > 1]\n",
    "        \n",
    "        return words\n",
    "\n",
    "    def train(self, X, y):\n",
    "        # Determine unique classes\n",
    "        self.classes = np.unique(y)\n",
    "        \n",
    "        # Calculate class probabilities\n",
    "        total_docs = len(y)\n",
    "        for cls in self.classes:\n",
    "            self.class_probs[cls] = np.sum(y == cls) / total_docs\n",
    "        \n",
    "        # Build vocabulary\n",
    "        for doc in X:\n",
    "            self.vocab.update(self._preprocess_text(doc))\n",
    "        \n",
    "        # Calculate word probabilities with add-1 smoothing\n",
    "        for cls in self.classes:\n",
    "            # Get documents of this class\n",
    "            cls_docs = X[y == cls]\n",
    "            \n",
    "            # Initialize word counts for this class\n",
    "            word_counts = {}\n",
    "            for doc in cls_docs:\n",
    "                words = self._preprocess_text(doc)\n",
    "                for word in words:\n",
    "                    word_counts[word] = word_counts.get(word, 0) + 1\n",
    "            \n",
    "            # Apply add-1 smoothing\n",
    "            self.word_probs[cls] = {}\n",
    "            total_words = sum(word_counts.values()) + len(self.vocab)\n",
    "            \n",
    "            for word in self.vocab:\n",
    "                count = word_counts.get(word, 0)\n",
    "                # Add-1 smoothing\n",
    "                self.word_probs[cls][word] = (count + 1) / total_words\n",
    "\n",
    "    def predict_proba(self, sentence):\n",
    "        words = self._preprocess_text(sentence)\n",
    "        \n",
    "        # Log-space calculations to avoid underflow\n",
    "        log_probs = {}\n",
    "        for cls in self.classes:\n",
    "            # Start with log of class probability\n",
    "            log_prob = math.log(self.class_probs[cls])\n",
    "            \n",
    "            # Add log probabilities of words\n",
    "            for word in words:\n",
    "                # Use log probability of word given class\n",
    "                if word in self.vocab:\n",
    "                    log_prob += math.log(self.word_probs[cls].get(word, 1e-10))\n",
    "            \n",
    "            log_probs[cls] = log_prob\n",
    "        \n",
    "        # Convert from log space\n",
    "        probs = {cls: math.exp(log_prob) for cls, log_prob in log_probs.items()}\n",
    "        \n",
    "        # Normalize probabilities\n",
    "        total = sum(probs.values())\n",
    "        probs = {cls: prob/total for cls, prob in probs.items()}\n",
    "        \n",
    "        return probs\n",
    "\n",
    "    def predict(self, sentence):\n",
    "        probs = self.predict_proba(sentence)\n",
    "        return max(probs, key=probs.get)\n",
    "\n",
    "def parse_arguments():\n",
    "    \"\"\"\n",
    "    Parse command line arguments with more robust error handling\n",
    "    \"\"\"\n",
    "    # Default values\n",
    "    algo = 0  # Default to Naive Bayes\n",
    "    train_size = 80  # Default train size percentage\n",
    "    \n",
    "    # Parse command line arguments\n",
    "    if len(sys.argv) == 3:\n",
    "        try:\n",
    "            algo = int(sys.argv[1])\n",
    "            if algo not in [0, 1]:\n",
    "                print(\"Invalid algorithm. Using default (0 - Naive Bayes).\")\n",
    "                algo = 0\n",
    "            \n",
    "            train_size = int(sys.argv[2])\n",
    "            if train_size < 50 or train_size > 80:\n",
    "                print(\"Train size must be between 50 and 80. Using default (80%).\")\n",
    "                train_size = 80\n",
    "        except ValueError:\n",
    "            print(\"Invalid arguments. Using defaults.\")\n",
    "    else:\n",
    "        print(\"Usage: python script.py <algorithm> <train_size>\")\n",
    "        print(\"Using default: Naive Bayes with 80% training size\")\n",
    "    \n",
    "    return algo, train_size\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    try:\n",
    "        # Load dataset using pandas\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Assuming the CSV has 'review' and 'sentiment' columns\n",
    "        texts = df['review'].values\n",
    "        labels = df['sentiment'].values\n",
    "        \n",
    "        return texts, labels\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    # Compute confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    sensitivity = recall_score(y_true, y_pred)\n",
    "    specificity = recall_score(y_true, y_pred, pos_label=0)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    \n",
    "    # Negative Predictive Value\n",
    "    npv = recall_score(y_true, y_pred, pos_label=0)\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f_score = f1_score(y_true, y_pred)\n",
    "    \n",
    "    return {\n",
    "        'true_positives': tp,\n",
    "        'true_negatives': tn,\n",
    "        'false_positives': fp,\n",
    "        'false_negatives': fn,\n",
    "        'sensitivity': sensitivity,\n",
    "        'specificity': specificity,\n",
    "        'precision': precision,\n",
    "        'negative_predictive_value': npv,\n",
    "        'accuracy': accuracy,\n",
    "        'f_score': f_score\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    # Parse arguments\n",
    "    algo, train_size = parse_arguments()\n",
    "    \n",
    "    # Load dataset \n",
    "    X, y = load_dataset('IMDB Dataset_cleaned.csv')  # Update with your actual file path\n",
    "    \n",
    "    # Split data\n",
    "    train_end = int(len(X) * (train_size/100))\n",
    "    test_start = int(len(X) * 0.8)  # Last 20% for test\n",
    "    \n",
    "    X_train, y_train = X[:train_end], y[:train_end]\n",
    "    X_test, y_test = X[test_start:], y[test_start:]\n",
    "    \n",
    "    # Print solution header\n",
    "    print(f\"Last Name, First Name, AXXXXXXXX solution:\")\n",
    "    print(f\"Training set size: {train_size} %\")\n",
    "    \n",
    "    # Train classifier\n",
    "    if algo == 0:  # Naive Bayes\n",
    "        print(\"Classifier type: Naive Bayes\")\n",
    "        print(\"Training classifier...\")\n",
    "        \n",
    "        clf = NaiveBayesClassifier()\n",
    "        clf.train(X_train, y_train)\n",
    "        \n",
    "        print(\"Testing classifier...\")\n",
    "        y_pred = [clf.predict(doc) for doc in X_test]\n",
    "        \n",
    "    else:  # Logistic Regression\n",
    "        print(\"Classifier type: Logistic Regression\")\n",
    "        print(\"Training classifier...\")\n",
    "        \n",
    "        # Vectorize text\n",
    "        vectorizer = CountVectorizer()\n",
    "        X_train_vec = vectorizer.fit_transform(X_train)\n",
    "        X_test_vec = vectorizer.transform(X_test)\n",
    "        \n",
    "        # Train logistic regression\n",
    "        clf = LogisticRegression()\n",
    "        clf.fit(X_train_vec, y_train)\n",
    "        \n",
    "        print(\"Testing classifier...\")\n",
    "        y_pred = clf.predict(X_test_vec)\n",
    "    \n",
    "    # Calculate and display metrics\n",
    "    metrics = calculate_metrics(y_test, y_pred)\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric.replace('_', ' ').title()}: {value:.4f}\")\n",
    "    \n",
    "    # Interactive classification\n",
    "    while True:\n",
    "        sentence = input(\"\\nEnter your sentence/document: \")\n",
    "        \n",
    "        if algo == 0:  # Naive Bayes\n",
    "            probs = clf.predict_proba(sentence)\n",
    "            predicted_class = clf.predict(sentence)\n",
    "            print(f\"was classified as {predicted_class}.\")\n",
    "            for cls, prob in probs.items():\n",
    "                print(f\"P({cls} | S) = {prob:.4f}\")\n",
    "        else:  # Logistic Regression\n",
    "            sentence_vec = vectorizer.transform([sentence])\n",
    "            predicted_class = clf.predict(sentence_vec)[0]\n",
    "            print(f\"was classified as {predicted_class}.\")\n",
    "        \n",
    "        cont = input(\"Do you want to enter another sentence [Y/N]? \").upper()\n",
    "        if cont != 'Y':\n",
    "            break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents: 50000\n",
      "Training documents: 40000\n",
      "Test documents: 10000\n",
      "\n",
      "Last Name, First Name, AXXXXXXXX solution:\n",
      "Training set size: 80 %\n",
      "Classifier type: naive_bayes\n",
      "\n",
      "Training classifier...\n",
      "Testing classifier...\n",
      "\n",
      "Model Performance Metrics:\n",
      "True Positives: 4077.0000\n",
      "True Negatives: 4403.0000\n",
      "False Positives: 590.0000\n",
      "False Negatives: 930.0000\n",
      "Sensitivity (Recall): 0.8143\n",
      "Specificity: 0.8818\n",
      "Precision: 0.8736\n",
      "Negative Predictive Value: 0.8256\n",
      "Accuracy: 0.8480\n",
      "F-score: 0.8429\n",
      "\n",
      "Last Name, First Name, AXXXXXXXX solution:\n",
      "Training set size: 80 %\n",
      "Classifier type: logistic_regression\n",
      "\n",
      "Training classifier...\n",
      "Testing classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\harsh\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Performance Metrics:\n",
      "True Positives: 0.0000\n",
      "True Negatives: 4993.0000\n",
      "False Positives: 0.0000\n",
      "False Negatives: 5007.0000\n",
      "Sensitivity (Recall): 0.0000\n",
      "Specificity: 1.0000\n",
      "Precision: 0.0000\n",
      "Negative Predictive Value: 0.4993\n",
      "Accuracy: 0.4993\n",
      "F-score: 0.0000\n",
      "\n",
      "Comparative Model Summary:\n",
      "\n",
      "Naive Bayes Performance:\n",
      "True Positives: 4077.0000\n",
      "True Negatives: 4403.0000\n",
      "False Positives: 590.0000\n",
      "False Negatives: 930.0000\n",
      "Sensitivity (Recall): 0.8143\n",
      "Specificity: 0.8818\n",
      "Precision: 0.8736\n",
      "Negative Predictive Value: 0.8256\n",
      "Accuracy: 0.8480\n",
      "F-score: 0.8429\n",
      "\n",
      "Logistic Regression Performance:\n",
      "True Positives: 0.0000\n",
      "True Negatives: 4993.0000\n",
      "False Positives: 0.0000\n",
      "False Negatives: 5007.0000\n",
      "Sensitivity (Recall): 0.0000\n",
      "Specificity: 1.0000\n",
      "Precision: 0.0000\n",
      "Negative Predictive Value: 0.4993\n",
      "Accuracy: 0.4993\n",
      "F-score: 0.0000\n",
      "was classified as 0\n",
      "P(Class 0 | S) = 0.5002\n",
      "P(Class 1 | S) = 0.4998\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, \n",
    "    roc_curve, \n",
    "    auc, \n",
    "    precision_recall_fscore_support, \n",
    "    accuracy_score\n",
    ")\n",
    "import math\n",
    "import re\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "class TextClassifier:\n",
    "    def __init__(self, algorithm='naive_bayes'):\n",
    "        self.algorithm = algorithm\n",
    "        self.vocab = set()\n",
    "        self.class_probs = {}\n",
    "        self.word_probs = {}\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        \n",
    "        # For Logistic Regression\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        self.lr_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "        \n",
    "    def preprocess_text(self, text):\n",
    "        # More efficient preprocessing\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', str(text).lower())\n",
    "        return text.split()\n",
    "    \n",
    "    def create_bow_vector(self, text, vocab=None):\n",
    "        if vocab is None:\n",
    "            vocab = self.vocab\n",
    "        \n",
    "        # Use Counter for more efficient word counting\n",
    "        word_counts = Counter(word for word in text if word in vocab)\n",
    "        return word_counts\n",
    "    \n",
    "    def train_naive_bayes(self, X_train, y_train):\n",
    "        # More efficient vocabulary and probability calculation\n",
    "        # Flatten the list of documents and get unique words\n",
    "        all_words = [word for doc in X_train for word in doc]\n",
    "        self.vocab = set(all_words)\n",
    "        \n",
    "        # Count documents per class\n",
    "        total_docs = len(y_train)\n",
    "        unique_classes = np.unique(y_train)\n",
    "        self.class_probs = {c: np.sum(y_train == c) / total_docs for c in unique_classes}\n",
    "        \n",
    "        # Efficient word probabilities calculation\n",
    "        self.word_probs = {c: Counter() for c in unique_classes}\n",
    "        class_word_counts = {c: Counter() for c in unique_classes}\n",
    "        \n",
    "        # Count words in each class\n",
    "        for doc, label in zip(X_train, y_train):\n",
    "            class_word_counts[label].update(doc)\n",
    "        \n",
    "        # Calculate word probabilities with add-1 smoothing\n",
    "        for cls in unique_classes:\n",
    "            total_words = sum(class_word_counts[cls].values())\n",
    "            for word in self.vocab:\n",
    "                word_count = class_word_counts[cls][word]\n",
    "                self.word_probs[cls][word] = (word_count + 1) / (total_words + len(self.vocab))\n",
    "    \n",
    "    def predict_naive_bayes(self, document):\n",
    "        # More numerically stable prediction\n",
    "        bow = self.create_bow_vector(document)\n",
    "        class_scores = {}\n",
    "        \n",
    "        for cls in self.class_probs.keys():\n",
    "            # Log-space calculation to prevent underflow\n",
    "            log_prob = math.log(self.class_probs[cls] + 1e-10)\n",
    "            for word, count in bow.items():\n",
    "                log_prob += count * math.log(self.word_probs[cls].get(word, 1e-10))\n",
    "            class_scores[cls] = log_prob\n",
    "        \n",
    "        # Softmax to get probabilities\n",
    "        max_score = max(class_scores.values())\n",
    "        exp_scores = {cls: math.exp(score - max_score) for cls, score in class_scores.items()}\n",
    "        total = sum(exp_scores.values())\n",
    "        probs = {cls: score/total for cls, score in exp_scores.items()}\n",
    "        \n",
    "        predicted_class = max(class_scores, key=class_scores.get)\n",
    "        return predicted_class, probs\n",
    "    \n",
    "    def train(self, X_train, y_train):\n",
    "        # Preprocess documents once\n",
    "        processed_train = [self.preprocess_text(doc) for doc in X_train]\n",
    "        \n",
    "        if self.algorithm == 'naive_bayes':\n",
    "            self.train_naive_bayes(processed_train, y_train)\n",
    "        else:\n",
    "            # Convert documents to BOW feature vectors\n",
    "            X_bow = np.array([\n",
    "                sum(self.create_bow_vector(doc, self.vocab).values()) \n",
    "                for doc in processed_train\n",
    "            ]).reshape(-1, 1)\n",
    "            \n",
    "            self.lr_model.fit(X_bow, y_train)\n",
    "    \n",
    "    def predict(self, document):\n",
    "        processed_doc = self.preprocess_text(document)\n",
    "        \n",
    "        if self.algorithm == 'naive_bayes':\n",
    "            return self.predict_naive_bayes(processed_doc)\n",
    "        else:\n",
    "            bow_vector = sum(self.create_bow_vector(processed_doc, self.vocab).values())\n",
    "            prediction = self.lr_model.predict([[bow_vector]])[0]\n",
    "            proba = self.lr_model.predict_proba([[bow_vector]])[0]\n",
    "            return prediction, dict(zip(self.lr_model.classes_, proba))\n",
    "\n",
    "    \n",
    "def load_dataset(filepath):\n",
    "    # This is a placeholder. Replace with actual data loading\n",
    "    df = pd.read_csv(filepath)\n",
    "    return df['review'].values, df['sentiment'].values\n",
    "\n",
    "def evaluate_model(y_true, y_pred, y_proba):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "    \n",
    "    metrics = {\n",
    "        'True Positives': tp,\n",
    "        'True Negatives': tn,\n",
    "        'False Positives': fp,\n",
    "        'False Negatives': fn,\n",
    "        'Sensitivity (Recall)': recall,\n",
    "        'Specificity': tn / (tn + fp),\n",
    "        'Precision': precision,\n",
    "        'Negative Predictive Value': tn / (tn + fn),\n",
    "        'Accuracy': accuracy_score(y_true, y_pred),\n",
    "        'F-score': f1\n",
    "    }\n",
    "    \n",
    "    return metrics, (tn, fp, fn, tp)\n",
    "\n",
    "def plot_roc_and_confusion_matrix(y_true_nb, y_pred_nb, y_proba_nb, \n",
    "                                   y_true_lr, y_pred_lr, y_proba_lr):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # ROC Curve subplot\n",
    "    plt.subplot(131)\n",
    "    fpr_nb, tpr_nb, _ = roc_curve(y_true_nb, y_proba_nb)\n",
    "    roc_auc_nb = auc(fpr_nb, tpr_nb)\n",
    "    \n",
    "    fpr_lr, tpr_lr, _ = roc_curve(y_true_lr, y_proba_lr)\n",
    "    roc_auc_lr = auc(fpr_lr, tpr_lr)\n",
    "    \n",
    "    plt.plot(fpr_nb, tpr_nb, label=f'Naive Bayes (AUC = {roc_auc_nb:.2f})')\n",
    "    plt.plot(fpr_lr, tpr_lr, label=f'Logistic Regression (AUC = {roc_auc_lr:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Naive Bayes Confusion Matrix\n",
    "    plt.subplot(132)\n",
    "    cm_nb = confusion_matrix(y_true_nb, y_pred_nb)\n",
    "    sns.heatmap(cm_nb, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Negative', 'Positive'], \n",
    "                yticklabels=['Negative', 'Positive'])\n",
    "    plt.title('Naive Bayes Confusion Matrix')\n",
    "    \n",
    "    # Logistic Regression Confusion Matrix\n",
    "    plt.subplot(133)\n",
    "    cm_lr = confusion_matrix(y_true_lr, y_pred_lr)\n",
    "    sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Negative', 'Positive'], \n",
    "                yticklabels=['Negative', 'Positive'])\n",
    "    plt.title('Logistic Regression Confusion Matrix')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('model_performance.png')\n",
    "    plt.close()\n",
    "\n",
    "def main():\n",
    "    # Set a time limit for training\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    MAX_TRAINING_TIME = 600  # 60 seconds max training time\n",
    "    \n",
    "    # Default values\n",
    "    algo = 0\n",
    "    train_size = 80\n",
    "    \n",
    "    # Parse command line arguments\n",
    "    if len(sys.argv) == 3:\n",
    "        algo = int(sys.argv[1])\n",
    "        train_size = int(sys.argv[2])\n",
    "    \n",
    "    # Validate train_size\n",
    "    if train_size < 50 or train_size > 80:\n",
    "        train_size = 80\n",
    "    \n",
    "    # Load dataset (replace with your actual dataset path)\n",
    "    X, y = load_dataset('IMDB Dataset_cleaned.csv')\n",
    "    \n",
    "    # # Limit dataset size if too large\n",
    "    # if len(X) > 1000:\n",
    "    #     print(f\"Large dataset detected. Sampling first 1000 documents.\")\n",
    "    #     indices = np.random.choice(len(X), 1000, replace=False)\n",
    "    #     X = X[indices]\n",
    "    #     y = y[indices]\n",
    "    \n",
    "    # Encode labels\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(y)\n",
    "    \n",
    "    # Split data\n",
    "    train_end = int(len(X) * (train_size/100))\n",
    "    test_start = int(len(X) * 0.8)\n",
    "    \n",
    "    X_train, X_test = X[:train_end], X[test_start:]\n",
    "    y_train, y_test = y[:train_end], y[test_start:]\n",
    "    \n",
    "    # Print dataset info\n",
    "    print(f\"Total documents: {len(X)}\")\n",
    "    print(f\"Training documents: {len(X_train)}\")\n",
    "    print(f\"Test documents: {len(X_test)}\")\n",
    "    \n",
    "    # Prepare results storage\n",
    "    results = {}\n",
    "    \n",
    "    # List of algorithms to compare\n",
    "    algorithms = ['naive_bayes', 'logistic_regression']\n",
    "    \n",
    "    # Compare both algorithms\n",
    "    for algorithm in algorithms:\n",
    "        # Check training time\n",
    "        if time.time() - start_time > MAX_TRAINING_TIME:\n",
    "            print(f\"Training exceeded {MAX_TRAINING_TIME} seconds. Stopping.\")\n",
    "            break\n",
    "        \n",
    "        classifier = TextClassifier(algorithm)\n",
    "        \n",
    "        print(f\"\\nLast Name, First Name, AXXXXXXXX solution:\")\n",
    "        print(f\"Training set size: {train_size} %\")\n",
    "        print(f\"Classifier type: {algorithm}\")\n",
    "        \n",
    "        print(\"\\nTraining classifier...\")\n",
    "        \n",
    "        # Add a progress indicator\n",
    "        try:\n",
    "            classifier.train(X_train, y_train)\n",
    "        except Exception as e:\n",
    "            print(f\"Training failed: {e}\")\n",
    "            continue\n",
    "        \n",
    "        print(\"Testing classifier...\")\n",
    "        \n",
    "        # Predictions and probabilities for each algorithm\n",
    "        y_pred = [classifier.predict(doc)[0] for doc in X_test]\n",
    "        \n",
    "        # For Naive Bayes, need to extract probability of positive class\n",
    "        if algorithm == 'naive_bayes':\n",
    "            y_proba = [classifier.predict(doc)[1][1] for doc in X_test]\n",
    "        else:\n",
    "            y_proba = classifier.lr_model.predict_proba(\n",
    "                [[sum(classifier.create_bow_vector(classifier.preprocess_text(doc), classifier.vocab).values())] for doc in X_test]\n",
    "            )[:, 1]\n",
    "        \n",
    "        # Evaluate\n",
    "        metrics, confusion = evaluate_model(y_test, y_pred, y_proba)\n",
    "        \n",
    "        # Store results\n",
    "        results[algorithm] = {\n",
    "            'metrics': metrics,\n",
    "            'confusion': confusion,\n",
    "            'predictions': y_pred,\n",
    "            'probabilities': y_proba\n",
    "        }\n",
    "        \n",
    "        # Print metrics\n",
    "        print(\"\\nModel Performance Metrics:\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    # Visualize both models' performance\n",
    "    plot_roc_and_confusion_matrix(\n",
    "        y_test, \n",
    "        results['naive_bayes']['predictions'], \n",
    "        results['naive_bayes']['probabilities'],\n",
    "        y_test, \n",
    "        results['logistic_regression']['predictions'], \n",
    "        results['logistic_regression']['probabilities']\n",
    "    )\n",
    "    \n",
    "    # Comparative summary\n",
    "    print(\"\\nComparative Model Summary:\")\n",
    "    for algo, res in results.items():\n",
    "        print(f\"\\n{algo.replace('_', ' ').title()} Performance:\")\n",
    "        for metric, value in res['metrics'].items():\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    # Interactive prediction for selected algorithm\n",
    "    algorithm = 'naive_bayes' if algo == 0 else 'logistic_regression'\n",
    "    classifier = TextClassifier(algorithm)\n",
    "    classifier.train(X_train, y_train)\n",
    "    \n",
    "    while True:\n",
    "        sentence = input(\"\\nEnter your sentence/document: \")\n",
    "        predicted_class, probs = classifier.predict(sentence)\n",
    "        \n",
    "        print(f\"was classified as {predicted_class}\")\n",
    "        print(f\"P(Class 0 | S) = {probs[0]:.4f}\")\n",
    "        print(f\"P(Class 1 | S) = {probs[1]:.4f}\")\n",
    "        \n",
    "        cont = input(\"\\nDo you want to enter another sentence [Y/N]? \").upper()\n",
    "        if cont != 'Y':\n",
    "            break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: python script.py <algorithm> <train_size>\n",
      "Using default: Naive Bayes with 80% training size\n",
      "Training set size: 80%\n",
      "Classifier type: naive bayes\n",
      "\n",
      "Training classifier...\n",
      "Testing classifier...\n",
      "\n",
      "Model Performance Metrics:\n",
      "True Positives: 4099.0000\n",
      "True Negatives: 4349.0000\n",
      "False Positives: 612.0000\n",
      "False Negatives: 940.0000\n",
      "Sensitivity (Recall): 0.8135\n",
      "Specificity: 0.8766\n",
      "Precision: 0.8701\n",
      "Negative Predictive Value: 0.8223\n",
      "Accuracy: 0.8448\n",
      "F-score: 0.8408\n",
      "was classified as negative\n",
      "P(Class negative | S) = 0.5010\n",
      "P(Class positive | S) = 0.4990\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, \n",
    "    roc_curve, \n",
    "    auc, \n",
    "    precision_recall_fscore_support, \n",
    "    accuracy_score\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "class TextClassifier:\n",
    "    def __init__(self, algorithm='naive_bayes'):\n",
    "        self.algorithm = algorithm\n",
    "        self.vocab = set()\n",
    "        self.class_probs = {}\n",
    "        self.word_probs = {}\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.vectorizer = None\n",
    "\n",
    "        # Logistic regression\n",
    "        self.lr_model = LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced')\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', str(text).lower())\n",
    "        return text.split()\n",
    "\n",
    "    def create_bow_vector(self, text, vocab=None):\n",
    "        if vocab is None:\n",
    "            vocab = self.vocab\n",
    "\n",
    "        word_counts = Counter(word for word in text if word in vocab)\n",
    "        return word_counts\n",
    "\n",
    "    def train_naive_bayes(self, X_train, y_train):\n",
    "        all_words = [word for doc in X_train for word in doc]\n",
    "        self.vocab = set(all_words)\n",
    "\n",
    "        total_docs = len(y_train)\n",
    "        unique_classes = np.unique(y_train)\n",
    "        self.class_probs = {c: np.sum(y_train == c) / total_docs for c in unique_classes}\n",
    "\n",
    "        self.word_probs = {c: Counter() for c in unique_classes}\n",
    "        class_word_counts = {c: Counter() for c in unique_classes}\n",
    "\n",
    "        for doc, label in zip(X_train, y_train):\n",
    "            class_word_counts[label].update(doc)\n",
    "\n",
    "        for cls in unique_classes:\n",
    "            total_words = sum(class_word_counts[cls].values())\n",
    "            for word in self.vocab:\n",
    "                word_count = class_word_counts[cls][word]\n",
    "                self.word_probs[cls][word] = (word_count + 1) / (total_words + len(self.vocab))\n",
    "\n",
    "    def predict_naive_bayes(self, document):\n",
    "        bow = self.create_bow_vector(document)\n",
    "        class_scores = {}\n",
    "\n",
    "        for cls in self.class_probs.keys():\n",
    "            log_prob = np.log(self.class_probs[cls] + 1e-10)\n",
    "            for word, count in bow.items():\n",
    "                log_prob += count * np.log(self.word_probs[cls].get(word, 1e-10))\n",
    "            class_scores[cls] = log_prob\n",
    "\n",
    "        max_score = max(class_scores.values())\n",
    "        exp_scores = {cls: np.exp(score - max_score) for cls, score in class_scores.items()}\n",
    "        total = sum(exp_scores.values())\n",
    "        probs = {cls: score / total for cls, score in exp_scores.items()}\n",
    "\n",
    "        predicted_class = max(class_scores, key=class_scores.get)\n",
    "        return predicted_class, probs\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        processed_train = [self.preprocess_text(doc) for doc in X_train]\n",
    "\n",
    "        if self.algorithm == 'naive_bayes':\n",
    "            self.train_naive_bayes(processed_train, y_train)\n",
    "        else:\n",
    "            # Using TfidfVectorizer for feature extraction\n",
    "            self.vectorizer = TfidfVectorizer()\n",
    "            X_bow = self.vectorizer.fit_transform([' '.join(doc) for doc in processed_train])\n",
    "\n",
    "            # Train logistic regression\n",
    "            self.lr_model.fit(X_bow, y_train)\n",
    "\n",
    "    def predict(self, document):\n",
    "        processed_doc = self.preprocess_text(document)\n",
    "\n",
    "        if self.algorithm == 'naive_bayes':\n",
    "            return self.predict_naive_bayes(processed_doc)\n",
    "        else:\n",
    "            X_bow = self.vectorizer.transform([' '.join(processed_doc)])\n",
    "            prediction = self.lr_model.predict(X_bow)[0]\n",
    "            proba = self.lr_model.predict_proba(X_bow)[0]\n",
    "            return prediction, dict(zip(self.lr_model.classes_, proba))\n",
    "\n",
    "\n",
    "def load_dataset(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    return df['review'].values, df['sentiment'].values\n",
    "\n",
    "\n",
    "def evaluate_model(y_true, y_pred, y_proba):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "\n",
    "    metrics = {\n",
    "        'True Positives': tp,\n",
    "        'True Negatives': tn,\n",
    "        'False Positives': fp,\n",
    "        'False Negatives': fn,\n",
    "        'Sensitivity (Recall)': recall,\n",
    "        'Specificity': tn / (tn + fp),\n",
    "        'Precision': precision,\n",
    "        'Negative Predictive Value': tn / (tn + fn),\n",
    "        'Accuracy': accuracy_score(y_true, y_pred),\n",
    "        'F-score': f1\n",
    "    }\n",
    "\n",
    "    return metrics, (tn, fp, fn, tp)\n",
    "\n",
    "\n",
    "def plot_roc_and_confusion_matrix(y_true_nb, y_pred_nb, y_proba_nb, \n",
    "                                   y_true_lr, y_pred_lr, y_proba_lr):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    plt.subplot(131)\n",
    "    fpr_nb, tpr_nb, _ = roc_curve(y_true_nb, y_proba_nb)\n",
    "    roc_auc_nb = auc(fpr_nb, tpr_nb)\n",
    "\n",
    "    fpr_lr, tpr_lr, _ = roc_curve(y_true_lr, y_proba_lr)\n",
    "    roc_auc_lr = auc(fpr_lr, tpr_lr)\n",
    "\n",
    "    plt.plot(fpr_nb, tpr_nb, label=f'Naive Bayes (AUC = {roc_auc_nb:.2f})')\n",
    "    plt.plot(fpr_lr, tpr_lr, label=f'Logistic Regression (AUC = {roc_auc_lr:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(132)\n",
    "    cm_nb = confusion_matrix(y_true_nb, y_pred_nb)\n",
    "    sns.heatmap(cm_nb, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Negative', 'Positive'], \n",
    "                yticklabels=['Negative', 'Positive'])\n",
    "    plt.title('Naive Bayes Confusion Matrix')\n",
    "\n",
    "    plt.subplot(133)\n",
    "    cm_lr = confusion_matrix(y_true_lr, y_pred_lr)\n",
    "    sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Negative', 'Positive'], \n",
    "                yticklabels=['Negative', 'Positive'])\n",
    "    plt.title('Logistic Regression Confusion Matrix')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('model_performance.png')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def parse_arguments():\n",
    "    algo = 0  # Default to Naive Bayes\n",
    "    train_size = 80  # Default train size: 80%\n",
    "\n",
    "    if len(sys.argv) == 3:\n",
    "        try:\n",
    "            algo = int(sys.argv[1])\n",
    "            if algo not in [0, 1]:\n",
    "                print(\"Invalid algorithm. Using default (0 - Naive Bayes).\")\n",
    "                algo = 0\n",
    "\n",
    "            train_size = int(sys.argv[2])\n",
    "            if train_size < 50 or train_size > 80:\n",
    "                print(\"Train size must be between 50 and 80. Using default (80%).\")\n",
    "                train_size = 80\n",
    "\n",
    "        except ValueError:\n",
    "            print(\"Invalid arguments. Using defaults.\")\n",
    "\n",
    "    else:\n",
    "        print(\"Usage: python script.py <algorithm> <train_size>\")\n",
    "        print(\"Using default: Naive Bayes with 80% training size\")\n",
    "\n",
    "    return algo, train_size / 100\n",
    "\n",
    "\n",
    "def main():\n",
    "    algo, train_size = parse_arguments()\n",
    "    file_path = 'IMDB Dataset_cleaned.csv'\n",
    "    X, y = load_dataset(file_path)\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(y)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_size, random_state=42)\n",
    "\n",
    "    print(f\"Training set size: {train_size * 100:.0f}%\")\n",
    "    results = {}\n",
    "    \n",
    "    if algo == 0:\n",
    "        classifier = TextClassifier('naive_bayes')\n",
    "        print(f\"Classifier type: naive bayes\")\n",
    "    \n",
    "        print(\"\\nTraining classifier...\")\n",
    "        \n",
    "        try:\n",
    "            classifier.train(X_train, y_train)\n",
    "        except Exception as e:\n",
    "            print(f\"Training failed: {e}\")\n",
    "            pass\n",
    "        \n",
    "        print(\"Testing classifier...\")\n",
    "        \n",
    "        y_pred = [classifier.predict(doc)[0] for doc in X_test]\n",
    "        \n",
    "        y_proba = [classifier.predict(doc)[1][1] for doc in X_test]\n",
    "        \n",
    "        metrics, confusion = evaluate_model(y_test, y_pred, y_proba)\n",
    "        \n",
    "        results['naive_bayes'] = {\n",
    "            'metrics': metrics,\n",
    "            'confusion': confusion,\n",
    "            'predictions': y_pred,\n",
    "            'probabilities': y_proba\n",
    "        }\n",
    "        \n",
    "    else:\n",
    "        classifier = TextClassifier('logistic_regression')\n",
    "        print(f\"Classifier type: logistic regression\")\n",
    "    \n",
    "        print(\"\\nTraining classifier...\")\n",
    "        \n",
    "        try:\n",
    "            classifier.train(X_train, y_train)\n",
    "        except Exception as e:\n",
    "            print(f\"Training failed: {e}\")\n",
    "            pass\n",
    "        \n",
    "        print(\"Testing classifier...\")\n",
    "        \n",
    "        y_pred = [classifier.predict(doc)[0] for doc in X_test]\n",
    "        \n",
    "        y_proba = classifier.lr_model.predict_proba(\n",
    "            [[sum(classifier.create_bow_vector(classifier.preprocess_text(doc), classifier.vocab).values())] for doc in X_test]\n",
    "        )[:, 1]\n",
    "        \n",
    "        metrics, confusion = evaluate_model(y_test, y_pred, y_proba)\n",
    "        \n",
    "        results['logistic_regression'] = {\n",
    "            'metrics': metrics,\n",
    "            'confusion': confusion,\n",
    "            'predictions': y_pred,\n",
    "            'probabilities': y_proba\n",
    "        }\n",
    "    \n",
    "    \n",
    "    print(\"\\nModel Performance Metrics:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    \n",
    "    while True:\n",
    "        sentence = input(\"\\nEnter your sentence/document: \")\n",
    "        predicted_class, probs = classifier.predict(sentence)\n",
    "        \n",
    "        if predicted_class == 0:\n",
    "            print(f\"was classified as negative\")\n",
    "        else:\n",
    "            print(f\"was classified as positive\")\n",
    "        print(f\"P(Class negative | S) = {probs[0]:.4f}\")\n",
    "        print(f\"P(Class positive | S) = {probs[1]:.4f}\")\n",
    "        \n",
    "        cont = input(\"\\nDo you want to enter another sentence [Y/N]? \").upper()\n",
    "        if cont != 'Y':\n",
    "            break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAApIAAAIjCAYAAACwHvu2AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABsuUlEQVR4nO3deXxM1/sH8M9kmckeSWSVIJI2hNhbYleRIJRKq/bYG40tsTWlao9SYitRSpQoaiuiCIIiVO1rlIYUidiSyL7d3x9+uV8jQXJrMiPzefc1r68599xzn3tlvnk899wzMkEQBBARERERlZGOugMgIiIioncTE0kiIiIikoSJJBERERFJwkSSiIiIiCRhIklEREREkjCRJCIiIiJJmEgSERERkSRMJImIiIhIEiaSRERERCQJE0mqkNq0aYM2bdq8tfGqV6+OAQMGvLXxtE1ERARkMhlu376t7lDUYt68eahRowZ0dXVRv379tz7+gAEDUL169bc+7rvq8OHDkMlkOHz4sLpDIarwmEiSShUlEH/99Ze6Q3mjEydOYOrUqUhJSVF3KPSWFRQUYM2aNWjTpg0sLS2hUChQvXp1DBw4UOU/m/v378eECRPQvHlzrFmzBrNnz1bp8crT7du3IZPJIJPJMHPmzBL79OnTBzKZDCYmJpKOsWHDBixcuPA/RElEqiTjd22TKkVERGDgwIE4ffo0GjduXG7Hzc3NBQDI5fJS7/P9999j/PjxiI+PL1bdycnJgY6ODvT19d9mmFqjoKAAeXl5UCgUkMlk5XrsrKwsdO/eHXv37kWrVq3QpUsXWFpa4vbt29i8eTNu3LiBhIQEODo6quT4X331FebNm4esrKwy/TyWRV5eHgoLC6FQKFQy/qvcvn0bzs7OMDAwQI0aNXDlyhWl7RkZGbC1tUVBQQF0dXWRnp5e5mN07twZly9fLlM1u7CwELm5uZDL5dDRYb2ESJX01B0AkSq87V/YqvwFXfRLz8DAQGXHeFFmZiaMjIzK5VhFdHV1oaurW67HLDJ+/Hjs3bsXYWFhGDNmjNK2b7/9FmFhYSo9fnJyMgwNDVWWRAJQ+z9wOnXqhG3btuHChQuoV6+e2P7bb78hNzcXHTp0wKFDh1QeR3Z2tpg8ltfniUjb8Z9qpBHOnTuHjh07wszMDCYmJmjXrh1OnjxZrN/FixfRunVrGBoawtHRETNnzsSaNWuKzb8raY7kkiVLULt2bRgZGcHCwgKNGzfGhg0bAABTp07F+PHjAQDOzs7i7bqiMUuaI5mSkoKgoCBUr14dCoUCjo6O6N+/Px49evTac5XJZBgxYgQiIyNRu3ZtKBQK7N27FwBw7949DBo0CLa2tlAoFKhduzZWr15dbIw7d+7g448/hrGxMWxsbBAUFIR9+/YVmxfWpk0b1KlTB2fOnEGrVq1gZGSEr7/+GsDzKuu3334LV1dXKBQKODk5YcKECcjJyVE6VnR0NFq0aIFKlSrBxMQEbm5u4hilubbAq+dILlu2TLwGDg4OCAwMLDa1oOgcrl69irZt28LIyAhVqlTB3LlzX3udAeDu3btYsWIF2rdvXyyJBJ4nuOPGjVOqRpbmZ7HofI4fP47g4GBYW1vD2NgYn3zyCR4+fCj2k8lkWLNmDTIyMsSfqYiICPGWcERERLGYZDIZpk6dKr5/9uwZxowZI/6c2djYoH379jh79qzYp6Q5khkZGRg7diycnJygUCjg5uaG77//Hi/fhCr6edyxYwfq1Kkj/twV/UyWhqenJ5ydnZX+zgEgMjISHTp0gKWlZbF9fvvtN/j6+sLBwQEKhQIuLi6YMWMGCgoKxD5t2rRBVFQU7ty5I16/ovMsmge5ceNGTJ48GVWqVIGRkRHS0tKKzZG8du0aDA0N0b9/f6UYjh07Bl1dXUycOLHU50pEyliRJLW7cuUKWrZsCTMzM0yYMAH6+vpYsWIF2rRpgyNHjqBJkyYAnidZbdu2hUwmQ0hICIyNjbFq1apSVQtXrlyJUaNG4dNPP8Xo0aORnZ2Nixcv4tSpU+jduze6d++OGzdu4JdffkFYWBgqV64MALC2ti5xvPT0dLRs2RLXrl3DoEGD0LBhQzx69Ag7d+7E3bt3xf1f5dChQ9i8eTNGjBiBypUro3r16njw4AGaNm0q/mK3trbG77//jsGDByMtLU1MhDIyMvDRRx8hMTERo0ePhp2dHTZs2ICYmJgSj/X48WN07NgRPXv2RN++fWFra4vCwkJ8/PHHOHbsGIYNG4ZatWrh0qVLCAsLw40bN7Bjxw7x76Zz586oW7cupk+fDoVCgZs3b+L48eOlvravMnXqVEybNg1eXl4YPnw44uLisHz5cpw+fRrHjx9XqrI9ffoUHTp0QPfu3dGjRw9s2bIFEydOhIeHBzp27PjKY/z+++/Iz89Hv379Xvv3UaS0P4tFRo4cCQsLC3z77be4ffs2Fi5ciBEjRmDTpk0AgHXr1uHHH3/En3/+iVWrVgEAmjVrVqpYigQEBGDLli0YMWIE3N3d8fjxYxw7dgzXrl1Dw4YNS9xHEAR8/PHHiImJweDBg1G/fn3s27cP48ePx71794pVYY8dO4Zt27bhyy+/hKmpKRYvXgw/Pz8kJCTAysqqVHH26tUL69evx5w5cyCTyfDo0SPs378f69atKzEpjYiIgImJCYKDg2FiYoJDhw5hypQpSEtLw7x58wAAkyZNQmpqKu7evSvG/PJcyxkzZkAul2PcuHHIyckpsfJbq1YtzJgxA+PHj8enn36Kjz/+GBkZGRgwYABq1qyJ6dOnl+ociagEApEKrVmzRgAgnD59+pV9unXrJsjlcuHWrVti2/379wVTU1OhVatWYtvIkSMFmUwmnDt3Tmx7/PixYGlpKQAQ4uPjxfbWrVsLrVu3Ft937dpVqF279mtjnTdvXrFxilSrVk3w9/cX30+ZMkUAIGzbtq1Y38LCwtceB4Cgo6MjXLlyRal98ODBgr29vfDo0SOl9p49ewrm5uZCZmamIAiCMH/+fAGAsGPHDrFPVlaWULNmTQGAEBMTI7a3bt1aACCEh4crjblu3TpBR0dH+OOPP5Taw8PDBQDC8ePHBUEQhLCwMAGA8PDhw1eeT2mubdHPQdG1TU5OFuRyueDt7S0UFBSI/ZYuXSoAEFavXl3sHH7++WexLScnR7CzsxP8/Pxee9ygoCABgNLPzOuU9mex6Hy8vLyU/r6DgoIEXV1dISUlRWzz9/cXjI2NlY4THx8vABDWrFlTLAYAwrfffiu+Nzc3FwIDA18bt7+/v1CtWjXx/Y4dOwQAwsyZM5X6ffrpp4JMJhNu3rypdDy5XK7UduHCBQGAsGTJktcet+g85s2bJ1y+fFkAIP5M/fDDD4KJiYmQkZFR4jUo+nl+0RdffCEYGRkJ2dnZYpuvr6/SuRWJiYkRAAg1atQoNlbRthc/CwUFBUKLFi0EW1tb4dGjR0JgYKCgp6f32v9vIqI3461tUquCggLs378f3bp1Q40aNcR2e3t79O7dG8eOHUNaWhoAYO/evfD09FRaPsXS0hJ9+vR543EqVaqEu3fv4vTp028l7q1bt6JevXr45JNPim0rzcMkrVu3hru7u/heEARs3boVXbp0gSAIePTokfjy8fFBamqqeCtz7969qFKlCj7++GNxfwMDAwwdOrTEYykUCgwcOFCp7ddff0WtWrVQs2ZNpWN99NFHACBWNytVqgTg+W3IwsLCEseXcm0PHDiA3NxcjBkzRulhiKFDh8LMzAxRUVFK/U1MTNC3b1/xvVwux4cffoh//vnntccp+tkxNTV9Y0xl+VksMmzYMKW/75YtW6KgoAB37tx54/FKq1KlSjh16hTu379f6n327NkDXV1djBo1Sql97NixEAQBv//+u1K7l5cXXFxcxPd169aFmZnZG6/vi2rXro26devil19+AfD8aeuuXbu+cj6uoaGh+Odnz57h0aNHaNmyJTIzM3H9+vVSH9ff319prFfR0dFBREQE0tPT0bFjRyxbtgwhISHl+hAgUUXERJLU6uHDh8jMzISbm1uxbbVq1UJhYSH+/fdfAM/nBbq6uhbrV1LbyyZOnAgTExN8+OGHeO+99xAYGKh0e7asbt26hTp16kje39nZWen9w4cPkZKSgh9//BHW1tZKr6IkMDk5GcDz6+Di4lIsYX3VdahSpUqx231///03rly5UuxY77//vtKxPv/8czRv3hxDhgyBra0tevbsic2bNysllVKubVGi9fLfu1wuR40aNYolYo6OjsXO18LCAk+fPn3tcczMzAA8T1TepCw/i0WqVq1aLCYAb4yrLObOnYvLly/DyckJH374IaZOnfrGBO/OnTtwcHAolkDXqlVL3P6il88DKN31fVnv3r3x66+/4ubNmzhx4sRrpzZcuXIFn3zyCczNzWFmZgZra2vxHwupqamlPubLn6XXcXFxwdSpU3H69GnUrl0b33zzTan3JaKScY4kaYVatWohLi4Ou3fvxt69e7F161YsW7YMU6ZMwbRp08o9npcrKEWJWd++feHv71/iPnXr1n0rxyo6noeHBxYsWFDiPk5OTuK+R48eRUxMDKKiorB3715s2rQJH330Efbv3w9dXd1yubaveuJbeMPqZTVr1gQAXLp0SSULgUuN61VV6xcfNCnSo0cPtGzZEtu3b8f+/fsxb948fPfdd9i2bdtr54eWhdTzeFmvXr0QEhKCoUOHwsrKCt7e3iX2S0lJQevWrWFmZobp06fDxcUFBgYGOHv2LCZOnPjK6ndJSlONfNH+/fsBAPfv38fjx49hZ2dXpv2JSBkTSVIra2trGBkZIS4urti269evQ0dHR0xqqlWrhps3bxbrV1JbSYyNjfH555/j888/R25uLrp3745Zs2YhJCQEBgYGZVrf0MXFBZcvXy51/zextraGqakpCgoK4OXl9dq+1apVw9WrVyEIglLMpb0OwPP4L1y4gHbt2r3xvHV0dNCuXTu0a9cOCxYswOzZszFp0iTExMSIsb7p2pZ0DgAQFxendBs5NzcX8fHxb7wGpdWxY0fo6upi/fr1b3zgpiw/i/9VUeXy5SfUX3VL3N7eHl9++SW+/PJLJCcno2HDhpg1a9YrE8lq1arhwIEDePbsmVJVsuiWcdH1f9uqVq2K5s2b4/Dhwxg+fDj09Er+FXP48GE8fvwY27ZtQ6tWrcT2+Pj4Yn3f5rqj4eHhiI6OxqxZsxAaGoovvvgCv/3221sbn0gb8dY2qZWuri68vb3x22+/KS0N8+DBA2zYsAEtWrQQb0/6+PggNjYW58+fF/s9efIEkZGRbzzO48ePld7L5XK4u7tDEATk5eUBeJ4MAcV/uZfEz88PFy5cwPbt24ttK2sVB3h+Hfz8/LB169YSE9QXl5Tx8fHBvXv3sHPnTrEtOzsbK1euLPXxevTogXv37pW4T1ZWFjIyMgA8v74vK6rsFS0TVJpr+zIvLy/I5XIsXrxY6Xr99NNPSE1Nha+vb6nP5XWcnJwwdOhQ7N+/H0uWLCm2vbCwEPPnz8fdu3fL9LP4X5mZmaFy5co4evSoUvuyZcuU3hcUFBS7zWtjYwMHB4diyzS9qFOnTigoKMDSpUuV2sPCwiCTyd5aJbMkM2fOxLfffouRI0e+sk9RBfTFv/vc3Nxi5w88/1yW5Vb3q8THx2P8+PHw8/PD119/je+//x47d+7Ezz///J/HJtJmrEhSuVi9enWJS4CMHj0aM2fOFNcq/PLLL6Gnp4cVK1YgJydHaa3ACRMmYP369Wjfvj1GjhwpLv9TtWpVPHny5LWVC29vb9jZ2aF58+awtbXFtWvXsHTpUvj6+ooVm0aNGgF4vuRIz549oa+vjy5duogJ5ovGjx+PLVu24LPPPsOgQYPQqFEjPHnyBDt37kR4eLjSosylNWfOHMTExKBJkyYYOnQo3N3d8eTJE5w9exYHDhwQk7ovvvgCS5cuRa9evTB69GjY29sjMjJSrPyVpoLTr18/bN68GQEBAYiJiUHz5s1RUFCA69evY/Pmzdi3bx8aN26M6dOn4+jRo/D19UW1atWQnJyMZcuWwdHRES1atCj1tX2ZtbU1QkJCMG3aNHTo0AEff/wx4uLisGzZMnzwwQdKD9b8V/Pnz8etW7cwatQobNu2DZ07d4aFhQUSEhLw66+/4vr16+jZsycAlPpn8W0YMmQI5syZgyFDhqBx48Y4evQobty4odTn2bNncHR0xKeffop69erBxMQEBw4cwOnTpzF//vxXjt2lSxe0bdsWkyZNwu3bt1GvXj3s378fv/32G8aMGaP0YM3b1rp1a7Ru3fq1fZo1awYLCwv4+/tj1KhRkMlkWLduXYn/CGvUqBE2bdqE4OBgfPDBBzAxMUGXLl3KFJMgCBg0aBAMDQ2xfPlyAM8/R1u3bsXo0aPh5eUFBweHMo1JRP9PHY+Kk/YoWiblVa9///1XEARBOHv2rODj4yOYmJgIRkZGQtu2bYUTJ04UG+/cuXNCy5YtBYVCITg6OgqhoaHC4sWLBQBCUlKS2O/l5X9WrFghtGrVSrCyshIUCoXg4uIijB8/XkhNTVUaf8aMGUKVKlUEHR0dpeVqXl7+RxCeLz00YsQIoUqVKoJcLhccHR0Ff3//Ysv3vAzAK5dzefDggRAYGCg4OTkJ+vr6gp2dndCuXTvhxx9/VOr3zz//CL6+voKhoaFgbW0tjB07Vti6dasAQDh58qTSdXjV0jy5ubnCd999J9SuXVtQKBSChYWF0KhRI2HatGnidTl48KDQtWtXwcHBQZDL5YKDg4PQq1cv4caNG2W6ti8v/1Nk6dKlQs2aNQV9fX3B1tZWGD58uPD06VOlPq86h5eXvHmd/Px8YdWqVULLli0Fc3NzQV9fX6hWrZowcODAYksDleZn8VXLWpW07ExJS98IwvPlbwYPHiyYm5sLpqamQo8ePYTk5GSl5X9ycnKE8ePHC/Xq1RNMTU0FY2NjoV69esKyZcveeC2ePXsmBAUFCQ4ODoK+vr7w3nvvCfPmzSu2PNWrfh5L+pl/2YvL/7xOSdfg+PHjQtOmTQVDQ0PBwcFBmDBhgrBv375i1y89PV3o3bu3UKlSJQGAeJ5F1/rXX38tdryX/x4WLVokABC2bt2q1C8hIUEwMzMTOnXq9Nr4iejV+F3b9M4bM2YMVqxYgfT0dLV9DZ8mWLhwIYKCgnD37l1UqVJF3eEQEZEWYCJJ75SsrCylpzQfP36M999/Hw0bNkR0dLQaIytfL1+H7OxsNGjQAAUFBcVujxIREakK50jSO8XT0xNt2rRBrVq18ODBA/z0009IS0vTuvXgunfvjqpVq6J+/fpITU3F+vXrcf369VI9eERERPS2MJGkd0qnTp2wZcsW/Pjjj5DJZGjYsCF++uknpSVEtIGPjw9WrVqFyMhIFBQUwN3dHRs3bsTnn3+u7tCIiEiL8NY2EREREUnCdSSJiIiISBImkkREREQkCRNJIiIiIpKkQj5sY9h0orpDICIVubxrqrpDICIVcbE2fHMnFTFsMEJlY2edW/rmTu8oViSJiIiISJIKWZEkIiIiKhMZa2tSMJEkIiIiksnUHcE7iek3EREREUnCiiQRERERb21LwqtGRERERJKwIklERETEOZKSsCJJRERERJKwIklERETEOZKS8KoRERERkSSsSBIRERFxjqQkTCSJiIiIeGtbEl41IiIiIpKEFUkiIiIi3tqWhBVJIiIiIpKEFUkiIiIizpGUhFeNiIiIiCRhRZKIiIiIcyQlYUWSiIiIiCRhRZKIiIiIcyQlYSJJRERExFvbkjD9JiIiItJQc+bMgUwmw5gxY8S27OxsBAYGwsrKCiYmJvDz88ODBw+U9ktISICvry+MjIxgY2OD8ePHIz8/X6nP4cOH0bBhQygUCri6uiIiIqLM8TGRJCIiIpLpqO4l0enTp7FixQrUrVtXqT0oKAi7du3Cr7/+iiNHjuD+/fvo3r27uL2goAC+vr7Izc3FiRMnsHbtWkRERGDKlClin/j4ePj6+qJt27Y4f/48xowZgyFDhmDfvn1lipGJJBEREZGGSU9PR58+fbBy5UpYWFiI7ampqfjpp5+wYMECfPTRR2jUqBHWrFmDEydO4OTJkwCA/fv34+rVq1i/fj3q16+Pjh07YsaMGfjhhx+Qm5sLAAgPD4ezszPmz5+PWrVqYcSIEfj0008RFhZWpjiZSBIRERGpsCKZk5ODtLQ0pVdOTs5rwwkMDISvry+8vLyU2s+cOYO8vDyl9po1a6Jq1aqIjY0FAMTGxsLDwwO2trZiHx8fH6SlpeHKlStin5fH9vHxEccoLSaSRERERCoUGhoKc3NzpVdoaOgr+2/cuBFnz54tsU9SUhLkcjkqVaqk1G5ra4ukpCSxz4tJZNH2om2v65OWloasrKxSnxuf2iYiIiLSUd1T2yEhIQgODlZqUygUJfb9999/MXr0aERHR8PAwEBlMb0trEgSERERqZBCoYCZmZnS61WJ5JkzZ5CcnIyGDRtCT08Penp6OHLkCBYvXgw9PT3Y2toiNzcXKSkpSvs9ePAAdnZ2AAA7O7tiT3EXvX9THzMzMxgaGpb63JhIEhEREWnIU9vt2rXDpUuXcP78efHVuHFj9OnTR/yzvr4+Dh48KO4TFxeHhIQEeHp6AgA8PT1x6dIlJCcni32io6NhZmYGd3d3sc+LYxT1KRqjtHhrm4iIiEhDFiQ3NTVFnTp1lNqMjY1hZWUltg8ePBjBwcGwtLSEmZkZRo4cCU9PTzRt2hQA4O3tDXd3d/Tr1w9z585FUlISJk+ejMDAQLESGhAQgKVLl2LChAkYNGgQDh06hM2bNyMqKqpM8TKRJCIiInqHhIWFQUdHB35+fsjJyYGPjw+WLVsmbtfV1cXu3bsxfPhweHp6wtjYGP7+/pg+fbrYx9nZGVFRUQgKCsKiRYvg6OiIVatWwcfHp0yxyARBEN7amWkIw6YT1R0CEanI5V1T1R0CEamIi3Xp5+a9bYZec1Q2dtaBr1Q2trpxjiQRERERScJb20REREQaMkfyXcOKJBERERFJwookERERURmX6aHneNWIiIiISBJWJImIiIg4R1ISJpJEREREvLUtCa8aEREREUnCiiQRERERb21LwookEREREUnCiiQRERER50hKwqtGRERERJKwIklERETEOZKSsCJJRERERJKwIklERETEOZKSMJEkIiIiYiIpCa8aEREREUnCiiQRERERH7aRhBVJIiIiIpKEFUkiIiIizpGUhFeNiIiIiCRhRZKIiIiIcyQlYUWSiIiIiCRhRZKIiIiIcyQlYSJJRERExFvbkjD9JiIiIiJJWJEkIiIirSdjRVISViSJiIiISBJWJImIiEjrsSIpDSuSRERERCQJK5JERERELEhKwookEREREUnCiiQRERFpPc6RlIaJJBEREWk9JpLS8NY2EREREUnCiiQRERFpPVYkpWFFkoiIiIgkYUWSiIiItB4rktKwIklEREREkrAiSURERMSCpCSsSBIRERGRJKxIEhERkdbjHElpWJEkIiIiIklYkSQiIiKtx4qkNEwkiYiISOsxkZSGt7aJiIiISBJWJImIiEjrsSIpDSuSRERERCQJK5JERERELEhKwookEREREUnCiiQRERFpPc6RlIYVSSIiIiKShBVJIiIi0nqsSErDiiQRERFpPZlMprJXWSxfvhx169aFmZkZzMzM4Onpid9//13c3qZNm2LjBwQEKI2RkJAAX19fGBkZwcbGBuPHj0d+fr5Sn8OHD6Nhw4ZQKBRwdXVFRESEpOvGiiQRERGRhnB0dMScOXPw3nvvQRAErF27Fl27dsW5c+dQu3ZtAMDQoUMxffp0cR8jIyPxzwUFBfD19YWdnR1OnDiBxMRE9O/fH/r6+pg9ezYAID4+Hr6+vggICEBkZCQOHjyIIUOGwN7eHj4+PmWKl4kkERERkYbc2e7SpYvS+1mzZmH58uU4efKkmEgaGRnBzs6uxP3379+Pq1ev4sCBA7C1tUX9+vUxY8YMTJw4EVOnToVcLkd4eDicnZ0xf/58AECtWrVw7NgxhIWFlTmR5K1tIiIiIhXKyclBWlqa0isnJ+eN+xUUFGDjxo3IyMiAp6en2B4ZGYnKlSujTp06CAkJQWZmprgtNjYWHh4esLW1Fdt8fHyQlpaGK1euiH28vLyUjuXj44PY2NgynxsTSSIiItJ6qpwjGRoaCnNzc6VXaGjoK2O5dOkSTExMoFAoEBAQgO3bt8Pd3R0A0Lt3b6xfvx4xMTEICQnBunXr0LdvX3HfpKQkpSQSgPg+KSnptX3S0tKQlZVVpuvGW9tEREREKhQSEoLg4GClNoVC8cr+bm5uOH/+PFJTU7Flyxb4+/vjyJEjcHd3x7Bhw8R+Hh4esLe3R7t27XDr1i24uLio7BxehYkkERERaT1VLv+jUChemzi+TC6Xw9XVFQDQqFEjnD59GosWLcKKFSuK9W3SpAkA4ObNm3BxcYGdnR3+/PNPpT4PHjwAAHFepZ2dndj2Yh8zMzMYGhqW/sSgQbe2//jjD/Tt2xeenp64d+8eAGDdunU4duyYmiMjIiIiUp/CwsJXzqk8f/48AMDe3h4A4OnpiUuXLiE5OVnsEx0dDTMzM/H2uKenJw4ePKg0TnR0tNI8zNLSiERy69at8PHxgaGhIc6dOyderNTUVPFRdSIiIiJV0ZR1JENCQnD06FHcvn0bly5dQkhICA4fPow+ffrg1q1bmDFjBs6cOYPbt29j586d6N+/P1q1aoW6desCALy9veHu7o5+/frhwoUL2LdvHyZPnozAwECxKhoQEIB//vkHEyZMwPXr17Fs2TJs3rwZQUFBZb5uGpFIzpw5E+Hh4Vi5ciX09fXF9ubNm+Ps2bNqjIyIiIi0gaYkksnJyejfvz/c3NzQrl07nD59Gvv27UP79u0hl8tx4MABeHt7o2bNmhg7diz8/Pywa9cucX9dXV3s3r0burq68PT0RN++fdG/f3+ldSednZ0RFRWF6Oho1KtXD/Pnz8eqVavKvPQPoCFzJOPi4tCqVati7ebm5khJSSn/gIiIiIjU4KeffnrlNicnJxw5cuSNY1SrVg179ux5bZ82bdrg3LlzZY7vZRpRkbSzs8PNmzeLtR87dgw1atRQQ0RERESkVWQqfFVgGpFIDh06FKNHj8apU6cgk8lw//59REZGYty4cRg+fLi6wyMiIiKiEmjEre2vvvoKhYWFaNeuHTIzM9GqVSsoFAqMGzcOI0eOVHd4REREVMGpcvmfikwjEkmZTIZJkyZh/PjxuHnzJtLT0+Hu7g4TExN1h0ZEREREr6ARieT69evRvXt3GBkZiWscEREREZUXViSl0Yg5kkFBQbCxsUHv3r2xZ88eFBQUqDskIiIiInoDjUgkExMTsXHjRshkMvTo0QP29vYIDAzEiRMn1B0aERERaQFNWUfyXaMRiaSenh46d+6MyMhIJCcnIywsDLdv30bbtm3V8gXkREREpGW4/I8kGjFH8kVGRkbw8fHB06dPcefOHVy7dk3dIRERERFRCTQmkczMzMT27dsRGRmJgwcPwsnJCb169cKWLVvUHRoRERFVcBX9FrSqaEQi2bNnT+zevRtGRkbo0aMHvvnmG3h6eqo7LCIiIiJ6DY1IJHV1dbF582b4+PhAV1dX3eEQERGRlmFFUhqNSCQjIyPVHQIRERERlZHaEsnFixdj2LBhMDAwwOLFi1/bd9SoUeUUFanbuH5tMCOwI5ZuPIbxC3cBABRyPcwZ5YvP2teDQl8PB07dwOh5O5D8JF3cr01jF3w7zAe1XeyQkZ2LyD1n8G34PhQUFIp9/NrVxXj/tnivamU8epqB8C0nEBZ5tNzPkUibXDp/Bls3rMXNuGt48vghJs9egGatPhK3C4KA9T8tx95d25Dx7BncPeojcNzXqOJUTexzM+4aVi9fiL+vX4GOji6at26HoSPHwdDISOzTqUX9YseeOHUOWnt1UOn5UcXBiqQ0akskw8LC0KdPHxgYGCAsLOyV/WQyGRNJLdGoliMGf9IEF/++r9Q+d0xndGxWC32+jkRaejbCxnXFxjn98NGw5QAAD1d77FgwCN9FHMLg6ZvgYG2GJRO7Q1dHByFLogAA3p5uWDOtJ4Ln/4YDp/5Gzeo2WBbih6ycPIRviS33cyXSFtlZWXB2fR/evt0wc1Jwse1bIiOwc8sGBE+aATv7Kli3ahm+Cf4S4eu3Qa5Q4PGjZHw95gu0aueDL4NDkJmRjhWL52HB7CmYNPN7pbGCvp6GRk2ai+9NTExVfn5E2k5tiWR8fHyJfybtZGwox5ppPfFl6FZ8NfB/1QozYwMM6PIBBkzZiCNnbgEAhs38FRc2jcOHtavizysJ+NSrLi7fTETo6oMAgH/uPsakpXuwfmYfzPopGumZuejdoQF2HbmCVdtPAQBu33+CeT/HYGy/NkwkiVToA88W+MCzRYnbBEHAjl8j0bP/UHi2bAsAGDt5Bnp/3A6xf8SgtVcH/Hn8KPT09PBlcAh0dJ4vfTxi3GQE+n+G+3cT4OBYVRzP2MQUllaVVX9SVCGxIimNRixIPn36dGRmZhZrz8rKwvTp09UQEZW3heO6Ye/x64g5fVOpvUHNKpDr6+HQ6b/Ftht3HiIh8SmaeDz/BaKQ6yE7N19pv6ycPBga6KNBTcfX9nG0rYSq9haqOCUieoOk+/fw9PEj1P+gidhmbGIKN3cPXLt8AQCQl5cHPX19MYkEAIVCAQC4cvGc0njLF4Sip28bjBnaB/t374AgCOVwFlRhcEFySTQikZw2bRrS09OLtWdmZmLatGmv3TcnJwdpaWlKL6Ew/7X7kGb5zKse6rs54Jvle4tts7MyRU5uPlLTs5Xak5+kw9bq+W2r6JM30NSjGnq0rwcdHRkcrM3w9aB2AAD7F/p0bVMHbRq7QCaTwdWpMkb3bqXUh4jK19MnjwAAFhZWSu2VLCzx9MljAEC9hh/g6ePH2LIhAnl5eXiWloY14c/n1T95/Ejcp++QL/HV9LmYFRaO5q298MOC2di55ZdyOhMi7aURT20LglBiSfnChQuwtLR87b6hoaHFkk3dKs2g71jyrRTSLI425pgX3AWdR61CTq60fwAc/PNvfL10DxZP7I6fvv0cOXkFmLP6IFo0qIHC/69IrP7tT9RwtMK27wdCX08HaRk5+GHzcXwztL3Yh4g0T7UargieNB2rls5HxIol0NHRQddPe8HC0go6sv/VQnoPGCb+2eX9msjOzsLWX9ai62e91RE2vYN4a1satSaSFhYW4heav//++0p/iQUFBUhPT0dAQMBrxwgJCUFwsPIEbhuv11cxSXM0qFkFtpamiI343wNVenq6aFHfGQGfeqLLmJ+gkOvB3MRAqSppY2mCB4+fie8X//IHFv/yB+wrm+LpsyxUs7fEjMCOiL/3ROwz+YffMWX5XthZmeLh0wy0/cAVAJT6EFH5sbB8Pp/x6dPHsKxsLbanPH2CGq7vi+/bendCW+9OePrkMQwMDCGTybB903rYOVR55dhu7nXwS8SPyMvNhb5crrqTINJyak0kFy5cCEEQMGjQIEybNg3m5ubiNrlcjurVq7/xG24UCoU4X6aITEcjCq1UCjF/3USj3guU2n6c/Bni7jzE/HWHcfdBKnLz8tH2A1fsiLkMAHivamVUtbfAqUsJxcZLfPQ8uezRvh7+TUrBubh7StsLCwXcf5gm9jl58Q4epWSo4MyI6E3sHKrAwqoyLvz1J1zeqwkAyMxIR9zVS/Dt9lmx/haWz2+B79+9A/pyORp80PSVY//zdxxMTM2YRFKpsSIpjVozLn9/fwCAs7MzmjVrBn19fXWGQ2qQnpmLq/88UGrLyM7Fk9RMsT1i12l8N6oznqRm4llGDhaM7YqTF+/gzyv/SySD+rTC/pM3UFgooGubOhjXvw36TopEYeHz29ZW5kb45CMPHD37DwzkeujfuTG6f1QX3l+Gl9/JEmmhrMxM3L/3v8/qg8R7uPX3dZiamsPGzh7dPuuDjWtXwsGpKmztq2Ddqh9gZWUtPsUNALu2bkStOvVgYGiEc6djsXrZQgwIGAUTUzMAwKljR/D06WPUrF0Xcrkc506fxKZ1P8GvV/9yP18ibaO2RDItLQ1mZs//T6BBgwbIyspCVlZWiX2L+pF2mrBwNwoLBfwS2g8K+f8vSD53u1Ifb083TBjwERT6erh0MxGfTfgZ+2PjlPr07dQIoSN9IZPJcOryHfgErsBfV++W56kQaZ2/r1/BV6OGiu9XLpkPAPDq2AXBk2bg0z4DkJ2dhSVzZyA9/RlqezTA9PnLIH/hTlPc1ctY/9NyZGVlwqmqM0aMn4x2HTqL23X19LB72yasXPw9BAhwqOKEoSPGocPH3cvvROmdx4KkNDJBTesj6OrqIjExETY2NtDR0SmxpFz0EE5BQUGZxjZsOvFthUlEGubyrqnqDoGIVMTF2lBtx3Yd97vKxr75fUeVja1uaqtIHjp0SHwiOyYmRl1hEBEREXGOpERqSyRbt25d4p+JiIiIyhvzSGk0YkHyvXv34tixY+L7H374AfXr10fv3r3x9OlTNUZGRERERK+iEYnk+PHjkZb2fEmWS5cuITg4GJ06dUJ8fHyxNSKJiIiI3raida1V8arINGLBxfj4eLi7uwMAtm7dii5dumD27Nk4e/YsOnXqpOboiIiIiKgkGlGRlMvlyMzMBAAcOHAA3t7eAABLS0uxUklERESkKjKZ6l4VmUZUJFu0aIHg4GA0b94cf/75JzZt2gQAuHHjBhwdHdUcHRERERGVRCMqkkuXLoWenh62bNmC5cuXo0qV59+f+vvvv6NDhw5qjo6IiIgqOh0dmcpeFZlGVCSrVq2K3bt3F2sPCwtTQzREREREVBoakUgCQEFBAXbs2IFr164BAGrXro2PP/4Yurq6ao6MiIiIKrqKPpdRVTQikbx58yY6deqEe/fuwc3NDQAQGhoKJycnREVFwcXFRc0REhERUUVW0ZfpURWNmCM5atQouLi44N9//8XZs2dx9uxZJCQkwNnZGaNGjVJ3eERERERUAo2oSB45cgQnT54Uv3sbAKysrDBnzhw0b95cjZERERGRNmBBUhqNqEgqFAo8e/asWHt6ejrkcrkaIiIiIiKiN9GIRLJz584YNmwYTp06BUEQIAgCTp48iYCAAHz88cfqDo+IiIgqOH5FojQakUguXrwYrq6uaNasGQwMDGBgYIDmzZvD1dUVixYtUnd4RERERFQCtc6RLCwsxLx587Bz507k5uaiW7du8Pf3h0wmQ61ateDq6qrO8IiIiEhLVPTKoaqoNZGcNWsWpk6dCi8vLxgaGmLPnj0wNzfH6tWr1RkWEREREZWCWm9t//zzz1i2bBn27duHHTt2YNeuXYiMjERhYaE6wyIiIiItI5Op7lWRqTWRTEhIQKdOncT3Xl5ekMlkuH//vhqjIiIiIm3Dh22kUWsimZ+fDwMDA6U2fX195OXlqSkiIiIiIiottc6RFAQBAwYMgEKhENuys7MREBAAY2NjsW3btm3qCI+IiIi0RAUvHKqMWhNJf3//Ym19+/ZVQyREREREVFZqTSTXrFmjzsMTERERAeDyP1JpxILkRERERPTuUWtFkoiIiEgTsCApDSuSRERERCQJK5JERESk9ThHUhpWJImIiIhIEiaSREREpPU05SsSly9fjrp168LMzAxmZmbw9PTE77//Lm7Pzs5GYGAgrKysYGJiAj8/Pzx48EBpjISEBPj6+sLIyAg2NjYYP3488vPzlfocPnwYDRs2hEKhgKurKyIiIiRdNyaSREREpPU05SsSHR0dMWfOHJw5cwZ//fUXPvroI3Tt2hVXrlwBAAQFBWHXrl349ddfceTIEdy/fx/du3cX9y8oKICvry9yc3Nx4sQJrF27FhEREZgyZYrYJz4+Hr6+vmjbti3Onz+PMWPGYMiQIdi3b1/Zr5sgCEKZ99Jwhk0nqjsEIlKRy7umqjsEIlIRF2tDtR27SegRlY19KqT1f9rf0tIS8+bNw6effgpra2ts2LABn376KQDg+vXrqFWrFmJjY9G0aVP8/vvv6Ny5M+7fvw9bW1sAQHh4OCZOnIiHDx9CLpdj4sSJiIqKwuXLl8Vj9OzZEykpKdi7d2+ZYmNFkoiIiLSeKm9t5+TkIC0tTemVk5PzxpgKCgqwceNGZGRkwNPTE2fOnEFeXh68vLzEPjVr1kTVqlURGxsLAIiNjYWHh4eYRAKAj48P0tLSxKpmbGys0hhFfYrGKAsmkkREREQqFBoaCnNzc6VXaGjoK/tfunQJJiYmUCgUCAgIwPbt2+Hu7o6kpCTI5XJUqlRJqb+trS2SkpIAAElJSUpJZNH2om2v65OWloasrKwynRuX/yEiIiKtp8rlf0JCQhAcHKzUplAoXtnfzc0N58+fR2pqKrZs2QJ/f38cOaK6W+//BRNJIiIiIhVSKBSvTRxfJpfL4erqCgBo1KgRTp8+jUWLFuHzzz9Hbm4uUlJSlKqSDx48gJ2dHQDAzs4Of/75p9J4RU91v9jn5Se9Hzx4ADMzMxgalm2eKm9tExERkdbTlOV/SlJYWIicnBw0atQI+vr6OHjwoLgtLi4OCQkJ8PT0BAB4enri0qVLSE5OFvtER0fDzMwM7u7uYp8XxyjqUzRGWbAiSURERKQhQkJC0LFjR1StWhXPnj3Dhg0bcPjwYezbtw/m5uYYPHgwgoODYWlpCTMzM4wcORKenp5o2rQpAMDb2xvu7u7o168f5s6di6SkJEyePBmBgYFiVTQgIABLly7FhAkTMGjQIBw6dAibN29GVFRUmeNlIklERERaT1O+IjE5ORn9+/dHYmIizM3NUbduXezbtw/t27cHAISFhUFHRwd+fn7IycmBj48Pli1bJu6vq6uL3bt3Y/jw4fD09ISxsTH8/f0xffp0sY+zszOioqIQFBSERYsWwdHREatWrYKPj0+Z4+U6kkT0TuE6kkQVlzrXkWzx/R8qG/vYuJYqG1vdOEeSiIiIiCThrW0iIiLSeppya/tdw4okEREREUnCiiQRERFpPVYkpWFFkoiIiIgkYUWSiIiItB4LktKwIklEREREkrAiSURERFqPcySlYSJJREREWo95pDS8tU1EREREkrAiSURERFqPt7alYUWSiIiIiCRhRZKIiIi0HguS0rAiSURERESSsCJJREREWk+HJUlJWJEkIiIiIklYkSQiIiKtx4KkNEwkiYiISOtx+R9peGubiIiIiCRhRZKIiIi0ng4LkpKwIklEREREkrAiSURERFqPcySlYUWSiIiIiCRhRZKIiIi0HguS0rAiSURERESSsCJJREREWk8GliSlYCJJREREWo/L/0jDW9tEREREJAkrkkRERKT1uPyPNKxIEhEREZEkrEgSERGR1mNBUhpWJImIiIhIElYkiYiISOvpsCQpSZkrkmvXrkVUVJT4fsKECahUqRKaNWuGO3fuvNXgiIiIiEhzlTmRnD17NgwNDQEAsbGx+OGHHzB37lxUrlwZQUFBbz1AIiIiIlWTyVT3qsjKfGv733//haurKwBgx44d8PPzw7Bhw9C8eXO0adPmbcdHREREpHJc/keaMlckTUxM8PjxYwDA/v370b59ewCAgYEBsrKy3m50RERERKSxylyRbN++PYYMGYIGDRrgxo0b6NSpEwDgypUrqF69+tuOj4iIiEjlWJCUpswVyR9++AGenp54+PAhtm7dCisrKwDAmTNn0KtXr7ceIBERERFppjJXJCtVqoSlS5cWa582bdpbCYiIiIiovHH5H2lKlUhevHix1APWrVtXcjBERERE9O4oVSJZv359yGQyCIJQ4vaibTKZDAUFBW81QCIiIiJVYz1SmlIlkvHx8aqOg4iIiIjeMaVKJKtVq6bqOIiIiIjUhutISlPmp7YBYN26dWjevDkcHBzEr0VcuHAhfvvtt7caHBEREVF50JGp7lWRlTmRXL58OYKDg9GpUyekpKSIcyIrVaqEhQsXvu34iIiIiEhDlTmRXLJkCVauXIlJkyZBV1dXbG/cuDEuXbr0VoMjIiIiKg8ymUxlr4qszIlkfHw8GjRoUKxdoVAgIyPjrQRFRERERJqvzImks7Mzzp8/X6x97969qFWr1tuIiYiIiKhcyWSqe1VkZf5mm+DgYAQGBiI7OxuCIODPP//EL7/8gtDQUKxatUoVMRIRERGRBipzIjlkyBAYGhpi8uTJyMzMRO/eveHg4IBFixahZ8+eqoiRiIiISKUq+lxGVSlzIgkAffr0QZ8+fZCZmYn09HTY2Ni87biIiIiISMNJSiQBIDk5GXFxcQCeZ/HW1tZvLSgiIiKi8lTR13tUlTI/bPPs2TP069cPDg4OaN26NVq3bg0HBwf07dsXqampqoiRiIiISKU0Zfmf0NBQfPDBBzA1NYWNjQ26desmFu6KtGnTptgxAgIClPokJCTA19cXRkZGsLGxwfjx45Gfn6/U5/Dhw2jYsCEUCgVcXV0RERFR5utW5kRyyJAhOHXqFKKiopCSkoKUlBTs3r0bf/31F7744osyB0BEREREzx05cgSBgYE4efIkoqOjkZeXB29v72JLLA4dOhSJiYnia+7cueK2goIC+Pr6Ijc3FydOnMDatWsRERGBKVOmiH3i4+Ph6+uLtm3b4vz58xgzZgyGDBmCffv2lSlemSAIQll2MDY2xr59+9CiRQul9j/++AMdOnTQiLUkDZtOVHcIRKQil3dNVXcIRKQiLtaGajv2oI2q+1KV1T09JO/78OFD2NjY4MiRI2jVqhWA5xXJ+vXrv/IbBX///Xd07twZ9+/fh62tLQAgPDwcEydOxMOHDyGXyzFx4kRERUXh8uXL4n49e/ZESkoK9u7dW+r4ylyRtLKygrm5ebF2c3NzWFhYlHU4IiIiogotJycHaWlpSq+cnJxS7Vs0bdDS0lKpPTIyEpUrV0adOnUQEhKCzMxMcVtsbCw8PDzEJBIAfHx8kJaWhitXroh9vLy8lMb08fFBbGxsmc6tzInk5MmTERwcjKSkJLEtKSkJ48ePxzfffFPW4YiIiIjUTkcmU9krNDQU5ubmSq/Q0NA3xlRYWIgxY8agefPmqFOnjtjeu3dvrF+/HjExMQgJCcG6devQt29fcXtSUpJSEglAfF+Uv72qT1paGrKyskp93Ur11HaDBg2UJov+/fffqFq1KqpWrQrg+YROhUKBhw8fcp4kERER0QtCQkIQHBys1KZQKN64X2BgIC5fvoxjx44ptQ8bNkz8s4eHB+zt7dGuXTvcunULLi4ubyfoUipVItmtWzcVh0FERESkPqpcj1yhUJQqcXzRiBEjsHv3bhw9ehSOjo6v7dukSRMAwM2bN+Hi4gI7Ozv8+eefSn0ePHgAALCzsxP/t6jtxT5mZmYwNCz9XNVSJZLffvttqQckIiIiImkEQcDIkSOxfft2HD58GM7Ozm/c5/z58wAAe3t7AICnpydmzZqF5ORk8UtjoqOjYWZmBnd3d7HPnj17lMaJjo6Gp6dnmeKVvCA5ERERUUWhKV+RGBgYiA0bNuC3336DqampOKfR3NwchoaGuHXrFjZs2IBOnTrBysoKFy9eRFBQEFq1aoW6desCALy9veHu7o5+/fph7ty5SEpKwuTJkxEYGChWRgMCArB06VJMmDABgwYNwqFDh7B582ZERUWVKd4yP2xTUFCA77//Hh9++CHs7OxgaWmp9CIiIiIiaZYvX47U1FS0adMG9vb24mvTpk0AALlcjgMHDsDb2xs1a9bE2LFj4efnh127dolj6OrqYvfu3dDV1YWnpyf69u2L/v37Y/r06WIfZ2dnREVFITo6GvXq1cP8+fOxatUq+Pj4lCneMlckp02bhlWrVmHs2LGYPHkyJk2ahNu3b2PHjh1KC10SERERvSs0pCCJNy3v7eTkhCNHjrxxnGrVqhW7df2yNm3a4Ny5c2WK72VlrkhGRkZi5cqVGDt2LPT09NCrVy+sWrUKU6ZMwcmTJ/9TMERERETqoMrlfyqyMieSSUlJ8PB4vkK7iYmJuFBm586dy3xfnYiIiIjeXWVOJB0dHZGYmAgAcHFxwf79+wEAp0+fLvOj7URERESaQCZT3asiK3Mi+cknn+DgwYMAgJEjR+Kbb77Be++9h/79+2PQoEFvPUAiIiIi0kxlfthmzpw54p8///xzVKtWDSdOnMB7772HLl26vNXgiIiIiMqDpiz/864pc0XyZU2bNkVwcDCaNGmC2bNnv42YiIiIiOgdIBPe9Jx5KV24cAENGzZEQUHB2xjuP8nOV3cERKQqFh+MUHcIRKQiWeeWqu3YI7dfU9nYSz6ppbKx1e0/VySJiIiISDvxKxKJiIhI63GOpDRMJImIiEjr6TCPlKTUiWRwcPBrtz98+PA/B0NERERE745SJ5Kl+S7GVq1a/adgiIiIiNSBFUlpSp1IxsTEqDIOIiIiInrHcI4kERERaT0+bCMNl/8hIiIiIklYkSQiIiKtxzmS0rAiSURERESSsCJJREREWo9TJKWRVJH8448/0LdvX3h6euLevXsAgHXr1uHYsWNvNTgiIiKi8qAjk6nsVZGVOZHcunUrfHx8YGhoiHPnziEnJwcAkJqaitmzZ7/1AImIiIhIM5U5kZw5cybCw8OxcuVK6Ovri+3NmzfH2bNn32pwREREROVBR4WviqzM5xcXF1fiN9iYm5sjJSXlbcRERERERO+AMieSdnZ2uHnzZrH2Y8eOoUaNGm8lKCIiIqLyJJOp7lWRlTmRHDp0KEaPHo1Tp05BJpPh/v37iIyMxLhx4zB8+HBVxEhEREREGqjMy/989dVXKCwsRLt27ZCZmYlWrVpBoVBg3LhxGDlypCpiJCIiIlKpiv50taqUOZGUyWSYNGkSxo8fj5s3byI9PR3u7u4wMTFRRXxEREREpKEkL0gul8vh7u7+NmMhIiIiUgsWJKUpcyLZtm1byF5ztQ8dOvSfAiIiIiIqb/yubWnKnEjWr19f6X1eXh7Onz+Py5cvw9/f/23FRUREREQarsyJZFhYWIntU6dORXp6+n8OiIiIiKi88WEbad7agut9+/bF6tWr39ZwRERERKThJD9s87LY2FgYGBi8reGIiIiIyg0LktKUOZHs3r270ntBEJCYmIi//voL33zzzVsLjIiIiIg0W5kTSXNzc6X3Ojo6cHNzw/Tp0+Ht7f3WAiMiIiIqL3xqW5oyJZIFBQUYOHAgPDw8YGFhoaqYiIiIiOgdUKaHbXR1deHt7Y2UlBQVhUNERERU/mQq/K8iK/NT23Xq1ME///yjiliIiIiI1EJHprpXRVbmRHLmzJkYN24cdu/ejcTERKSlpSm9iIiIiEg7lHqO5PTp0zF27Fh06tQJAPDxxx8rfVWiIAiQyWQoKCh4+1ESERERqVBFrxyqSqkTyWnTpiEgIAAxMTGqjIeIiIiI3hGlTiQFQQAAtG7dWmXBEBEREamDjCuSS1KmOZK8yERERERUpEzrSL7//vtvTCafPHnynwIiIiIiKm+cIylNmRLJadOmFftmGyIiIiLSTmVKJHv27AkbGxtVxUJERESkFpy9J02pE0nOjyQiIqKKSod5jiSlftim6KltIiIiIiKgDBXJwsJCVcZBREREpDZ82EaaMn9FIhERERERUMaHbYiIiIgqIk6RlIYVSSIiIiKShBVJIiIi0no6YElSClYkiYiIiEgSViSJiIhI63GOpDSsSBIREZHW05Gp7lUWoaGh+OCDD2BqagobGxt069YNcXFxSn2ys7MRGBgIKysrmJiYwM/PDw8ePFDqk5CQAF9fXxgZGcHGxgbjx49Hfn6+Up/Dhw+jYcOGUCgUcHV1RURERNmvW5n3ICIiIiKVOHLkCAIDA3Hy5ElER0cjLy8P3t7eyMjIEPsEBQVh165d+PXXX3HkyBHcv38f3bt3F7cXFBTA19cXubm5OHHiBNauXYuIiAhMmTJF7BMfHw9fX1+0bdsW58+fx5gxYzBkyBDs27evTPHKhAr4lTXZ+W/uQ0TvJosPRqg7BCJSkaxzS9V27B9P3lHZ2P4N7JCTk6PUplAooFAo3rjvw4cPYWNjgyNHjqBVq1ZITU2FtbU1NmzYgE8//RQAcP36ddSqVQuxsbFo2rQpfv/9d3Tu3Bn379+Hra0tACA8PBwTJ07Ew4cPIZfLMXHiRERFReHy5cvisXr27ImUlBTs3bu31OfGiiQRERGRCoWGhsLc3FzpFRoaWqp9U1NTAQCWlpYAgDNnziAvLw9eXl5in5o1a6Jq1aqIjY0FAMTGxsLDw0NMIgHAx8cHaWlpuHLlitjnxTGK+hSNUVp82IaIiIi0nioftgkJCUFwcLBSW2mqkYWFhRgzZgyaN2+OOnXqAACSkpIgl8tRqVIlpb62trZISkoS+7yYRBZtL9r2uj5paWnIysqCoaFhqc6NiSQRERGRCpX2NvbLAgMDcfnyZRw7dkwFUb0dvLVNREREWk9HJlPZS4oRI0Zg9+7diImJgaOjo9huZ2eH3NxcpKSkKPV/8OAB7OzsxD4vP8Vd9P5NfczMzEpdjQSYSBIRERFpDEEQMGLECGzfvh2HDh2Cs7Oz0vZGjRpBX18fBw8eFNvi4uKQkJAAT09PAICnpycuXbqE5ORksU90dDTMzMzg7u4u9nlxjKI+RWOUFm9tExERkdbTlAXJAwMDsWHDBvz2228wNTUV5zSam5vD0NAQ5ubmGDx4MIKDg2FpaQkzMzOMHDkSnp6eaNq0KQDA29sb7u7u6NevH+bOnYukpCRMnjwZgYGB4i32gIAALF26FBMmTMCgQYNw6NAhbN68GVFRUWWKl4kkERERaT1NuUW7fPlyAECbNm2U2tesWYMBAwYAAMLCwqCjowM/Pz/k5OTAx8cHy5YtE/vq6upi9+7dGD58ODw9PWFsbAx/f39Mnz5d7OPs7IyoqCgEBQVh0aJFcHR0xKpVq+Dj41OmeLmOJBG9U7iOJFHFpc51JCNOJ6hs7AEfVFXZ2OrGiiQRERFpPZmm3Nt+x2hKJZeIiIiI3jGsSBIREZHWYz1SGlYkiYiIiEgSViSJiIhI60ldOFzbsSJJRERERJKwIklERERaj/VIaZhIEhERkdbjnW1peGubiIiIiCRhRZKIiIi0Hhckl4YVSSIiIiKShBVJIiIi0nqsrEnD60ZEREREkrAiSURERFqPcySlYUWSiIiIiCRhRZKIiIi0HuuR0rAiSURERESSsCJJREREWo9zJKVhIklERERaj7dopeF1IyIiIiJJWJEkIiIircdb29KwIklEREREkrAiSURERFqP9UhpWJEkIiIiIklYkSQiIiKtxymS0rAiSURERESSsCJJREREWk+HsyQlYSJJREREWo+3tqXhrW0iIiIikkRjEsk//vgDffv2haenJ+7duwcAWLduHY4dO6bmyIiIiKiik6nwv4pMIxLJrVu3wsfHB4aGhjh37hxycnIAAKmpqZg9e7aaoyMiIiKikmhEIjlz5kyEh4dj5cqV0NfXF9ubN2+Os2fPqjEyIiIi0gYymepeFZlGJJJxcXFo1apVsXZzc3OkpKSUf0BERERE9EYakUja2dnh5s2bxdqPHTuGGjVqqCEiIiIi0iY6kKnsVZFpRCI5dOhQjB49GqdOnYJMJsP9+/cRGRmJcePGYfjw4eoOj4iIiIhKoBHrSH711VcoLCxEu3btkJmZiVatWkGhUGDcuHEYOXKkusMjIiKiCq6iz2VUFZkgCIK6gyiSm5uLmzdvIj09He7u7jAxMZE0Tnb+Ww6MiDSGxQcj1B0CEalI1rmlajv2/msPVTa2dy1rlY2tbhpxa3v9+vXIzMyEXC6Hu7s7PvzwQ8lJJBERERGVD41IJIOCgmBjY4PevXtjz549KCgoUHdIREREpEW4ILk0GpFIJiYmYuPGjZDJZOjRowfs7e0RGBiIEydOqDs0IiIiInoFjUgk9fT00LlzZ0RGRiI5ORlhYWG4ffs22rZtCxcXF3WHR0RERBWcjkx1r4pMI57afpGRkRF8fHzw9OlT3LlzB9euXVN3SERERERUAo1JJDMzM7F9+3ZERkbi4MGDcHJyQq9evbBlyxZ1h0ZEREQVXEWfy6gqGpFI9uzZE7t374aRkRF69OiBb775Bp6enuoOi4iIiIheQyMSSV1dXWzevBk+Pj7Q1dVVdzhERESkZbgguTQakUhGRkaqOwQiIiLSYry1LY3aEsnFixdj2LBhMDAwwOLFi1/bd9SoUeUUFRERERGVltq+ItHZ2Rl//fUXrKys4Ozs/Mp+MpkM//zzT5nG5lckElVc/IpEoopLnV+RePTGE5WN3ep9S5WNrW5qq0jGx8eX+GciIiIiejdoxILk06dPR2ZmZrH2rKwsTJ8+XQ0RERERkTbhVyRKoxGJ5LRp05Cenl6sPTMzE9OmTVNDRERERET0JhqRSAqCAFkJz91fuHABlpYVd14BvdqDBw8QMnEcWjVrgg8b1oVfty64cvmSuF0QBPywZBHatW6BDxvWxbDBA3Dnzm2lMUYFBsCnXRt80MAD7Vq3wNdfjUdy8oNyPhMiKjJuYHtknVuKeeP8AAAWZkZYMPEzXNj+DZ7ELsCNPdMxf8KnMDMxUNrPyc4C2xYH4PGJBbhzMBSzx3SDru7/fn01q18Dh9YE4W7Md3gSuwDnt03GyD5ty/Xc6N0nk6nuVZGpdfkfCwsLyGQyyGQyvP/++0rJZEFBAdLT0xEQEKDGCEkd0lJTMaBvLzT+sAl+CF8JC0sLJNy5AzMzc7HPmp9W4pfIdZgxew6qVHHED0sWYfiwwdi+cw8UCgUA4IMPm2LIsABUtrZG8oMHWPD9XIwLGo2fIzeq69SItFYj96oY7NccF2/cFdvsrc1hb22OkLDtuPZPEqraW2LJpJ6wtzZH7/E/AQB0dGTYtng4HjxOQ9sB82FnbY5VM/ohL78A3y7dBQDIyMpF+KajuHTjHjKyctGsgQuWTu6JjKxcrN52XC3nS6Qt1PbUNgCsXbsWgiBg0KBBWLhwIczN/5coyOVyVK9eXdI33PCp7XfbwgXf4/y5s4hYt6HE7YIgwKtNS/QfMBD+AwcDAJ49e4aPWjXD9Flz0LGTb4n7HT50EGNGBeL0uUvQ19dXWfykWnxq+91jbChH7C9fYXToJnw1pAMuxt3F+O+3lti3u1cDrJ7VH1bNxqKgoBDezd2xbVEAanhPQvKTZwCAIZ+2wMxRXeH00VfIyy8ocZyN3w9BRlYuBn/zs8rOi94+dT61ffzvpyobu/l7FmXqf/ToUcybNw9nzpxBYmIitm/fjm7duonbBwwYgLVr1yrt4+Pjg71794rvnzx5gpEjR2LXrl3Q0dGBn58fFi1aBBMTE7HPxYsXERgYiNOnT8Pa2hojR47EhAkTyhSrWiuS/v7+AJ4vBdSsWTP+cicAwJGYQ2jWvAXGBY3CX3+dho2NLT7v2Rt+n/UAANy7exePHj1Ek6bNxH1MTU3hUbceLl44V2IimZqSgqioXahXvwF/zojK2cKQz7H3j8uIORWHr4Z0eG1fM1MDpGVko6CgEADQpK4zLt+8LyaRABB94hqWTOoJdxd7XIi7W2yMem6OaFKvBqYt2/V2T4QqNB0NugedkZGBevXqYdCgQejevXuJfTp06IA1a9aI74vuxhXp06cPEhMTER0djby8PAwcOBDDhg3Dhg3PizRpaWnw9vaGl5cXwsPDcenSJQwaNAiVKlXCsGHDSh2rRnyzTevWrcU/Z2dnIzc3V2m7mZnZK/fNyclBTk6OUpugqyh2Qendcffuv9i86Rf08x+IwcMCcOXSJXwXOhP6+vr4uNsnePToIQDAqrKV0n5WVlZ49OiRUlvY/HnY+EsksrOyULdefSxZFl5u50FEwGc+jVC/phNa9J37xr5WlYwRMrQjVm89IbbZWpkh+fEzpX7JT9Keb6tsBsT9r/3m3hmobGECPV1dzFyxBxHbY9/OSRCVs44dO6Jjx46v7aNQKGBnZ1fitmvXrmHv3r04ffo0GjduDABYsmQJOnXqhO+//x4ODg6IjIxEbm4uVq9eDblcjtq1a+P8+fNYsGBBmRJJjXjYJjMzEyNGjICNjQ2MjY1hYWGh9Hqd0NBQmJubK73mfRdaTpGTKhQWCqjlXhujxgSjVi13fNrjc3T/tAd+3Vz2uY0DBg3Gpi3bEb5yNXR0dDA5ZCLUOJuDSKs42lbCvPF+GDgpAjm5r59zZGpsgO2Lh+PaP4mYuSJK0vHaDVqI5n3mYeSsjRjRuy16dGgkaRzSTjIVvnJycpCWlqb0erkIVlaHDx+GjY0N3NzcMHz4cDx+/FjcFhsbi0qVKolJJAB4eXlBR0cHp06dEvu0atUKcrlc7OPj44O4uDg8fVr62/wakUiOHz8ehw4dwvLly6FQKLBq1SpMmzYNDg4O+Pnn189vCQkJQWpqqtJr/MSQcoqcVMHa2ho1XFyU2mrUqIHExPsAgMqVrQEAjx89Vurz+PFjVK5cWanNwsIS1as7w7NZc8z9Pgx/HD2CixfOqy54IhI1qFUVtlZmiN0wEc9OL8Kz04vQqvF7+LJXazw7vQg6Os9vJZoYKbDzhy/xLDMbnwevRH5+oTjGg8dpsLEyVRrXxvL5XaoHj9KU2u/cf4wrN+9jzfYTWBJ5CJO+6KTiMyQqnZKKXqGh0oteHTp0wM8//4yDBw/iu+++w5EjR9CxY0cUFDyfM5yUlAQbGxulffT09GBpaYmkpCSxj62trVKfovdFfUpDI25t79q1Cz///DPatGmDgQMHomXLlnB1dUW1atUQGRmJPn36vHJfhaL4bWw+bPNuq9+gIW6/9G1Hd27fhoNDFQBAFUdHVK5sjVOnYlGzVi0AQHp6Oi5dvIDPPu/1ynELC5//cnp56gQRqUbMn3Fo9OkspbYfp/VFXPwDzI+IRmGhAFNjA+xaFoic3Hx8OmZFscrlqYvxmDjYB9YWJnj49Pl6w+2a1kTqsyxc++fVv+x0dGRQyDXiVxy9K1Q4RTIkJATBwcFKbf9lCl7Pnj3FP3t4eKBu3bpwcXHB4cOH0a5dO8njSqERn7InT56gRo0aAJ7Ph3zy5Pn3XbZo0QLDhw9XZ2ikBn37+8O/by+s+jEc3j4dcfnSRWzZshlTpj7/liOZTIY+/fpj5YrlqFa1Gqo4Pl/+x9rGBh+18wIAXLx4AVcuXUKDho1gZm6GfxMSsGzJIjg5VUW9+g3UeXpEWiM9MwdXbyUqtWVk5eJJagau3kqEqbEBdi8LhKGBHAMnrYWZsQHMjJ+vIfnwaToKCwUciL2Ga/8k4aeZ/pi0aAdsrczwbWBnrNh8FLl5z5POL3q0wr9JTxB3+/k6sS0aumJMv3ZY9suR8j1holcoqej1NtWoUQOVK1fGzZs30a5dO9jZ2SE5OVmpT35+Pp48eSLOq7Szs8ODB8prKxe9f9Xcy5JoRCJZo0YNxMfHo2rVqqhZsyY2b96MDz/8ELt27UKlSpXUHR6VszoedbFg0VIsXrgAK5b/gCqOjpgw8Wv4dv5Y7DNw8NDnX6E5dQqePUtDg4aNsGzFKvGDamhggIMH9mP5D0uQlZWJytbWaN6iJeZ+8aXSfBAiUp/6NZ3wYV1nAMDVXVOVtrl1moKExCcoLBTgN3o5Fn3dE4cjxiIjOweRu/7E9OX/m0epoyPD9JEfo3oVK+TnF+Kfu48wefFvWLWFa0hS6b3LX2V49+5dPH78GPb29gAAT09PpKSk4MyZM2jU6Plc4UOHDqGwsBBNmjQR+0yaNAl5eXniaibR0dFwc3N74/MpL1LrOpJFwsLCoKuri1GjRuHAgQPo0qULBEFAXl4eFixYgNGjR5dpPN7aJqq4uI4kUcWlznUkT91KVdnYTVzM39zpBenp6bh58yYAoEGDBliwYAHatm0LS0tLWFpaYtq0afDz84OdnR1u3bqFCRMm4NmzZ7h06ZJYUOnYsSMePHiA8PBwcfmfxo0bi8v/pKamws3NDd7e3pg4cSIuX76MQYMGISwsrExPbWtEIvmyO3fu4MyZM3B1dUXdunXLvD8TSaKKi4kkUcWlzkTyz39Ul0h+WKNsieThw4fRtm3xr/n09/fH8uXL0a1bN5w7dw4pKSlwcHCAt7c3ZsyYofTwzJMnTzBixAilBckXL178ygXJK1eujJEjR2LixIllilUjE8n/iokkUcXFRJKo4lJnInlahYnkB2VMJN8lGjFHcvHixSW2y2QyGBgYwNXVFa1atYKurm45R0ZEREREr6IRiWRYWBgePnyIzMxMcYLn06dPYWRkBBMTEyQnJ6NGjRqIiYmBk5OTmqMlIiKiCufdfdZGrTRiQfLZs2fjgw8+wN9//43Hjx/j8ePHuHHjBpo0aYJFixYhISEBdnZ2CAoKUneoRERERPT/NGKOpIuLC7Zu3Yr69esrtZ87dw5+fn74559/cOLECfj5+SExMbHkQV7AOZJEFRfnSBJVXOqcI/lXfNqbO0nU2NlMZWOrm0ZUJBMTE5GfXzz7y8/PF7+mx8HBAc+ePSvv0IiIiIjoFTQikWzbti2++OILnDt3Tmw7d+4chg8fjo8++ggAcOnSJTg7O6srRCIiIqrAZDLVvSoyjUgkf/rpJ1haWqJRo0bi1wg1btwYlpaW+OmnnwAAJiYmmD9/vpojJSIiIqIiGvHUtp2dHaKjo3H9+nXcuHEDAODm5gY3NzexT0kLcxIRERG9DRW8cKgyGpFIFqlRowZkMhlcXFygp6dRoREREVFFxkxSEo24tZ2ZmYnBgwfDyMgItWvXRkJCAgBg5MiRmDNnjpqjIyIiIqKSaEQiGRISggsXLuDw4cMwMDAQ2728vLBp0yY1RkZERETaQKbC/yoyjbh/vGPHDmzatAlNmzaF7IXHm2rXro1bt26pMTIiIiIiehWNSCQfPnwIGxubYu0ZGRlKiSURERGRKjDdkEYjbm03btwYUVFR4vui5HHVqlXw9PRUV1hERERE9BoaUZGcPXs2OnbsiKtXryI/Px+LFi3C1atXceLECRw5ckTd4REREVEFx4KkNBpRkWzRogXOnz+P/Px8eHh4YP/+/bCxsUFsbCwaNWqk7vCIiIiIqAQaUZEEABcXF6xcuVLdYRAREZE2YklSErUmkjo6Om98mEYmkyE/P7+cIiIiIiJtVNGX6VEVtSaS27dvf+W22NhYLF68GIWFheUYERERERGVlloTya5duxZri4uLw1dffYVdu3ahT58+mD59uhoiIyIiIm3C5X+k0YiHbQDg/v37GDp0KDw8PJCfn4/z589j7dq1qFatmrpDIyIiIqISqD2RTE1NxcSJE+Hq6oorV67g4MGD2LVrF+rUqaPu0IiIiEhLyFT4qsjUemt77ty5+O6772BnZ4dffvmlxFvdRERERKSZZIIgCOo6uI6ODgwNDeHl5QVdXd1X9tu2bVuZxs3mQ95EFZbFByPUHQIRqUjWuaVqO/ble+kqG7tOFROVja1uaq1I9u/fn9+lTURERPSOUmsiGRERoc7DExEREQHgOpJSqf1hGyIiIiJ6N2nMVyQSERERqQtn2knDRJKIiIi0HvNIaXhrm4iIiIgkYUWSiIiIiCVJSViRJCIiIiJJWJEkIiIircflf6RhRZKIiIiIJGFFkoiIiLQel/+RhhVJIiIiIpKEFUkiIiLSeixISsNEkoiIiIiZpCS8tU1EREREkrAiSURERFqPy/9Iw4okEREREUnCiiQRERFpPS7/Iw0rkkREREQkCSuSREREpPVYkJSGFUkiIiIikoQVSSIiIiKWJCVhIklERERaj8v/SMNb20REREQkCSuSREREpPW4/I80rEgSERERkSSsSBIREZHWY0FSGlYkiYiIiEgSViSJiIiIWJKUhBVJIiIiIg1y9OhRdOnSBQ4ODpDJZNixY4fSdkEQMGXKFNjb28PQ0BBeXl74+++/lfo8efIEffr0gZmZGSpVqoTBgwcjPT1dqc/FixfRsmVLGBgYwMnJCXPnzi1zrEwkiYiISOvJVPhfWWVkZKBevXr44YcfStw+d+5cLF68GOHh4Th16hSMjY3h4+OD7OxssU+fPn1w5coVREdHY/fu3Th69CiGDRsmbk9LS4O3tzeqVauGM2fOYN68eZg6dSp+/PHHsl03QRCEMp+hhsvOV3cERKQqFh+MUHcIRKQiWeeWqu3YCU9yVDZ2VUuF5H1lMhm2b9+Obt26AXhejXRwcMDYsWMxbtw4AEBqaipsbW0RERGBnj174tq1a3B3d8fp06fRuHFjAMDevXvRqVMn3L17Fw4ODli+fDkmTZqEpKQkyOVyAMBXX32FHTt24Pr166WOjxVJIiIiIhXKyclBWlqa0isnR1riGh8fj6SkJHh5eYlt5ubmaNKkCWJjYwEAsbGxqFSpkphEAoCXlxd0dHRw6tQpsU+rVq3EJBIAfHx8EBcXh6dPn5Y6HiaSREREpPVkKnyFhobC3Nxc6RUaGiopzqSkJACAra2tUrutra24LSkpCTY2Nkrb9fT0YGlpqdSnpDFePEZp8KltIiIiIhUKCQlBcHCwUptCIf12tyZhIklERERaT5VfkahQKN5a4mhnZwcAePDgAezt7cX2Bw8eoH79+mKf5ORkpf3y8/Px5MkTcX87Ozs8ePBAqU/R+6I+pcFb20RERETvCGdnZ9jZ2eHgwYNiW1paGk6dOgVPT08AgKenJ1JSUnDmzBmxz6FDh1BYWIgmTZqIfY4ePYq8vDyxT3R0NNzc3GBhYVHqeJhIEhEREal0lmTZpKen4/z58zh//jyA5w/YnD9/HgkJCZDJZBgzZgxmzpyJnTt34tKlS+jfvz8cHBzEJ7tr1aqFDh06YOjQofjzzz9x/PhxjBgxAj179oSDgwMAoHfv3pDL5Rg8eDCuXLmCTZs2YdGiRcVuwb8Jb20TERERaZC//voLbdu2Fd8XJXf+/v6IiIjAhAkTkJGRgWHDhiElJQUtWrTA3r17YWBgIO4TGRmJESNGoF27dtDR0YGfnx8WL14sbjc3N8f+/fsRGBiIRo0aoXLlypgyZYrSWpOlwXUkieidwnUkiSouda4jeS8lV2VjV6kkf3OndxQrkkRERKT1+FXb0nCOJBERERFJwookERERaT1VLv9TkbEiSURERESSsCJJREREWk/GWZKSsCJJRERERJKwIklERETEgqQkrEgSERERkSSsSBIREZHWY0FSGiaSREREpPW4/I80vLVNRERERJKwIklERERaj8v/SMOKJBERERFJwookEREREQuSkrAiSURERESSsCJJREREWo8FSWlYkSQiIiIiSViRJCIiIq3HdSSlYSJJREREWo/L/0jDW9tEREREJAkrkkRERKT1eGtbGlYkiYiIiEgSJpJEREREJAkTSSIiIiKShHMkiYiISOtxjqQ0rEgSERERkSSsSBIREZHW4zqS0jCRJCIiIq3HW9vS8NY2EREREUnCiiQRERFpPRYkpWFFkoiIiIgkYUWSiIiIiCVJSViRJCIiIiJJWJEkIiIircflf6RhRZKIiIiIJGFFkoiIiLQe15GUhhVJIiIiIpKEFUkiIiLSeixISsNEkoiIiIiZpCS8tU1EREREkrAiSURERFqPy/9Iw4okEREREUnCiiQRERFpPS7/Iw0rkkREREQkiUwQBEHdQRBJlZOTg9DQUISEhEChUKg7HCJ6i/j5JtJ8TCTpnZaWlgZzc3OkpqbCzMxM3eEQ0VvEzzeR5uOtbSIiIiKShIkkEREREUnCRJKIiIiIJGEiSe80hUKBb7/9lhPxiSogfr6JNB8ftiEiIiIiSViRJCIiIiJJmEgSERERkSRMJImIiIhIEiaS9E46fPgwZDIZUlJSXtuvevXqWLhwYbnERETqxc87UfljIkkqNWDAAMhkMshkMsjlcri6umL69OnIz8//T+M2a9YMiYmJMDc3BwBERESgUqVKxfqdPn0aw4YN+0/HIqL/fZbnzJmj1L5jxw7IZLJyjYWfdyLNwUSSVK5Dhw5ITEzE33//jbFjx2Lq1KmYN2/efxpTLpfDzs7ujb/ArK2tYWRk9J+ORUTPGRgY4LvvvsPTp0/VHUqJ+HknKn9MJEnlFAoF7OzsUK1aNQwfPhxeXl7YuXMnnj59iv79+8PCwgJGRkbo2LEj/v77b3G/O3fuoEuXLrCwsICxsTFq166NPXv2AFC+tX348GEMHDgQqampYvVz6tSpAJRvdfXu3Ruff/65Umx5eXmoXLkyfv75ZwBAYWEhQkND4ezsDENDQ9SrVw9btmxR/UUiegd4eXnBzs4OoaGhr+xz7NgxtGzZEoaGhnBycsKoUaOQkZEhbk9MTISvry8MDQ3h7OyMDRs2FLslvWDBAnh4eMDY2BhOTk748ssvkZ6eDgD8vBNpGCaSVO4MDQ2Rm5uLAQMG4K+//sLOnTsRGxsLQRDQqVMn5OXlAQACAwORk5ODo0eP4tKlS/juu+9gYmJSbLxmzZph4cKFMDMzQ2JiIhITEzFu3Lhi/fr06YNdu3aJv5AAYN++fcjMzMQnn3wCAAgNDcXPP/+M8PBwXLlyBUFBQejbty+OHDmioqtB9O7Q1dXF7NmzsWTJEty9e7fY9lu3bqFDhw7w8/PDxYsXsWnTJhw7dgwjRowQ+/Tv3x/379/H4cOHsXXrVvz4449ITk5WGkdHRweLFy/GlStXsHbtWhw6dAgTJkwAwM87kcYRiFTI399f6Nq1qyAIglBYWChER0cLCoVC6NatmwBAOH78uNj30aNHgqGhobB582ZBEATBw8NDmDp1aonjxsTECACEp0+fCoIgCGvWrBHMzc2L9atWrZoQFhYmCIIg5OXlCZUrVxZ+/vlncXuvXr2Ezz//XBAEQcjOzhaMjIyEEydOKI0xePBgoVevXlJOn6jCePGz3LRpU2HQoEGCIAjC9u3bhaJfJYMHDxaGDRumtN8ff/wh6OjoCFlZWcK1a9cEAMLp06fF7X///bcAQPycluTXX38VrKysxPf8vBNpDj21ZrGkFXbv3g0TExPk5eWhsLAQvXv3Rvfu3bF79240adJE7GdlZQU3Nzdcu3YNADBq1CgMHz4c+/fvh5eXF/z8/FC3bl3Jcejp6aFHjx6IjIxEv379kJGRgd9++w0bN24EANy8eROZmZlo37690n65ublo0KCB5OMSVTTfffcdPvroo2KVwAsXLuDixYuIjIwU2wRBQGFhIeLj43Hjxg3o6emhYcOG4nZXV1dYWFgojXPgwAGEhobi+vXrSEtLQ35+PrKzs5GZmVnqOZD8vBOVDyaSpHJt27bF8uXLIZfL4eDgAD09PezcufON+w0ZMgQ+Pj6IiorC/v37ERoaivnz52PkyJGSY+nTpw9at26N5ORkREdHw9DQEB06dAAA8RZYVFQUqlSporQfv+uX6H9atWoFHx8fhISEYMCAAWJ7eno6vvjiC4waNarYPlWrVsWNGzfeOPbt27fRuXNnDB8+HLNmzYKlpSWOHTuGwYMHIzc3t0wP0/DzTqR6TCRJ5YyNjeHq6qrUVqtWLeTn5+PUqVNo1qwZAODx48eIi4uDu7u72M/JyQkBAQEICAhASEgIVq5cWWIiKZfLUVBQ8MZYmjVrBicnJ2zatAm///47PvvsM+jr6wMA3N3doVAokJCQgNatW/+XUyaq8ObMmYP69evDzc1NbGvYsCGuXr1a7PNexM3NDfn5+Th37hwaNWoE4Hll8MWnwM+cOYPCwkLMnz8fOjrPp/Fv3rxZaRx+3ok0BxNJUov33nsPXbt2xdChQ7FixQqYmpriq6++QpUqVdC1a1cAwJgxY9CxY0e8//77ePr0KWJiYlCrVq0Sx6tevTrS09Nx8OBB1KtXD0ZGRq+sXPTu3Rvh4eG4ceMGYmJixHZTU1OMGzcOQUFBKCwsRIsWLZCamorjx4/DzMwM/v7+b/9CEL2jPDw80KdPHyxevFhsmzhxIpo2bYoRI0ZgyJAhMDY2xtWrVxEdHY2lS5eiZs2a8PLywrBhw7B8+XLo6+tj7NixMDQ0FJfycnV1RV5eHpYsWYIuXbrg+PHjCA8PVzo2P+9EGkTdkzSpYntxgv7Lnjx5IvTr108wNzcXDA0NBR8fH+HGjRvi9hEjRgguLi6CQqEQrK2thX79+gmPHj0SBKH4wzaCIAgBAQGClZWVAED49ttvBUFQnnxf5OrVqwIAoVq1akJhYaHStsLCQmHhwoWCm5uboK+vL1hbWws+Pj7CkSNH/vO1IHqXlfRZjo+PF+RyufDir5I///xTaN++vWBiYiIYGxsLdevWFWbNmiVuv3//vtCxY0dBoVAI1apVEzZs2CDY2NgI4eHhYp8FCxYI9vb24v8v/Pzzz/y8E2komSAIghrzWCIi0mJ3796Fk5MTDhw4gHbt2qk7HCIqIyaSRERUbg4dOoT09HR4eHggMTEREyZMwL1793Djxg1x/iIRvTs4R5KIiMpNXl4evv76a/zzzz8wNTVFs2bNEBkZySSS6B3FiiQRERERScKvSCQiIiIiSZhIEhEREZEkTCSJiIiISBImkkREREQkCRNJIiIiIpKEiSQRSTZgwAB069ZNfN+mTRuMGTOm3OM4fPgwZDIZUlJSVHaMl89VivKIk4ioPDGRJKpgBgwYAJlMBplMBrlcDldXV0yfPh35+fkqP/a2bdswY8aMUvUt76SqevXqWLhwYbkci4hIW3BBcqIKqEOHDlizZg1ycnKwZ88eBAYGQl9fHyEhIcX65ubmQi6Xv5XjWlpavpVxiIjo3cCKJFEFpFAoYGdnh2rVqmH48OHw8vLCzp07AfzvFu2sWbPg4OAANzc3AMC///6LHj16oFKlSrC0tETXrl1x+/ZtccyCggIEBwejUqVKsLKywoQJE/Dy9xm8fGs7JycHEydOhJOTExQKBVxdXfHTTz/h9u3baNu2LQDAwsICMpkMAwYMAAAUFhYiNDQUzs7OMDQ0RL169bBlyxal4+zZswfvv/8+DA0N0bZtW6U4pSgoKMDgwYPFY7q5uWHRokUl9p02bRqsra1hZmaGgIAA5ObmittKE/uL7ty5gy5dusDCwgLGxsaoXbs29uzZ85/OhYioPLEiSaQFDA0N8fjxY/H9wYMHYWZmhujoaADPv7bOx8cHnp6e+OOPP6Cnp4eZM2eiQ4cOuHjxIuRyOebPn4+IiAisXr0atWrVwvz587F9+3Z89NFHrzxu//79ERsbi8WLF6NevXqIj4/Ho0eP4OTkhK1bt8LPzw9xcXEwMzODoaEhACA0NBTr169HeHg43nvvPRw9ehR9+/aFtbU1WrdujX///Rfdu3dHYGAghg0bhr/++gtjx479T9ensLAQjo6O+PXXX2FlZYUTJ05g2LBhsLe3R48ePZSum4GBAQ4fPozbt29j4MCBsLKywqxZs0oV+8sCAwORm5uLo0ePwtjYGFevXoWJicl/OhcionIlEFGF4u/vL3Tt2lUQBEEoLCwUoqOjBYVCIYwbN07cbmtrK+Tk5Ij7rFu3TnBzcxMKCwvFtpycHMHQ0FDYt2+fIAiCYG9vL8ydO1fcnpeXJzg6OorHEgRBaN26tTB69GhBEAQhLi5OACBER0eXGGdMTIwAQHj69KnYlp2dLRgZGQknTpxQ6jt48GChV69egiAIQkhIiODu7q60feLEicXGelm1atWEsLCwV25/WWBgoODn5ye+9/f3FywtLYWMjAyxbfny5YKJiYlQUFBQqthfPmcPDw9h6tSppY6JiEjTsCJJVAHt3r0bJiYmyMvLQ2FhIXr37o2pU6eK2z08PJTmRV64cAE3b96Eqamp0jjZ2dm4desWUlNTkZiYiCZNmojb9PT00Lhx42K3t4ucP38eurq6JVbiXuXmzZvIzMxE+/btldpzc3PRoEEDAMC1a9eU4gAAT0/PUh/jVX744QesXr0aCQkJyMrKQm5uLurXr6/Up169ejAyMlI6bnp6Ov7991+kp6e/MfaXjRo1CsOHD8f+/fvh5eUFPz8/1K1b9z+fCxFReWEiSVQBtW3bFsuXL4dcLoeDgwP09JQ/6sbGxkrv09PT0ahRI0RGRhYby9raWlIMRbeqyyI9PR0AEBUVhSpVqihtUygUkuIojY0bN2LcuHGYP38+PD09YWpqinnz5uHUqVOlHkNK7EOGDIGPjw+ioqKwf/9+hIaGYv78+Rg5cqT0kyEiKkdMJIkqIGNjY7i6upa6f8OGDbFp0ybY2NjAzMysxD729vY4deoUWrVqBQDIz8/HmTNn0LBhwxL7e3h4oLCwEEeOHIGXl1ex7UUV0YKCArHN3d0dCoUCCQkJr6xk1qpVS3xwqMjJkyfffJKvcfz4cTRr1gxffvml2Hbr1q1i/S5cuICsrCwxST558iRMTEzg5OQES0vLN8ZeEicnJwQEBCAgIAAhISFYuXIlE0kiemfwqW0iQp8+fVC5cmV07doVf/zxB+Lj43H48GGMGjUKd+/eBQCMHj0ac+bMwY4dO3D9+nV8+eWXr10Dsnr16vD398egQYOwY8cOcczNmzcDAKpVqwaZTIbdu3fj4cOHSE9Ph6mpKcaNG4egoCCsXbsWt27dwtmzZ7FkyRKsXbsWABAQEIC///4b48ePR1xcHDZs2ICIiIhSnee9e/dw/vx5pdfTp0/x3nvv4a+//sK+fftw48YNfPPNNzh9+nSx/XNzczF48GBcvXoVe/bswbfffosRI0ZAR0enVLG/bMyYMdi3bx/i4+Nx9uxZxMTEoFatWqU6FyIijaDuSZpE9Ha9+LBNWbYnJiYK/fv3FypXriwoFAqhRo0awtChQ4XU1FRBEJ4/XDN69GjBzMxMqFSpkhAcHCz079//lQ/bCIIgZGVlCUFBQYK9vb0gl8sFV1dXYfXq1eL26dOnC3Z2doJMJhP8/f0FQXj+gNDChQsFNzc3QV9fX7C2thZ8fHyEI0eOiPvt2rVLcHV1FRQKhdCyZUth9erVpXrYBkCx17p164Ts7GxhwIABgrm5uVCpUiVh+PDhwldffSXUq1ev2HWbMmWKYGVlJZiYmAhDhw4VsrOzxT5viv3lh21GjBghuLi4CAqFQrC2thb69esnPHr06JXnQESkaWSC8IqZ8kREREREr8Fb20REREQkCRNJIiIiIpKEiSQRERERScJEkoiIiIgkYSJJRERERJIwkSQiIiIiSZhIEhEREZEkTCSJiIiISBImkkREREQkCRNJIiIiIpKEiSQRERERSfJ/zTuWrDN/C94AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Confusion matrix values\n",
    "true_positives = 4099\n",
    "true_negatives = 4203\n",
    "false_positives = 603\n",
    "false_negatives = 1095\n",
    "\n",
    "# Create the confusion matrix\n",
    "conf_matrix = np.array([\n",
    "    [true_positives, false_negatives],\n",
    "    [false_positives, true_negatives]\n",
    "])\n",
    "\n",
    "# Define labels\n",
    "labels = ['Positive', 'Negative']\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "\n",
    "plt.title('Logistic regression Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
